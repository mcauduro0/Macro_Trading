---
phase: 02-connectors
plan: 02
type: execute
wave: 2
depends_on: ["02-01"]
files_modified:
  - src/connectors/bcb_sgs.py
  - src/connectors/fred.py
  - src/connectors/__init__.py
  - tests/connectors/test_bcb_sgs.py
  - tests/connectors/test_fred.py
autonomous: true
requirements:
  - CONN-02
  - CONN-03
  - DATA-01
  - DATA-02
  - TEST-02

must_haves:
  truths:
    - "BCB SGS connector fetches series data with correct DD/MM/YYYY date parsing and period-decimal value parsing"
    - "BCB SGS connector stores observations with release_time populated (datetime.now for SGS since no explicit publish timestamp)"
    - "BCB SGS connector splits date ranges exceeding 10 years into chunks (BCB API limit since March 2025)"
    - "BCB SGS SERIES_REGISTRY contains all ~50 series from the Fase0 guide (inflation, activity, monetary, external, fiscal)"
    - "FRED connector fetches series with missing-value handling (skips observations where value == '.')"
    - "FRED connector supports revision tracking via realtime_start/realtime_end and revision_number field"
    - "FRED SERIES_REGISTRY contains all ~50 US macro series from the Fase0 guide"
    - "Both connectors use ON CONFLICT DO NOTHING via _bulk_insert and can be re-run without creating duplicates"
    - "Tests mock HTTP responses via respx and verify parsing, missing value handling, and date format conversion"
  artifacts:
    - path: "src/connectors/bcb_sgs.py"
      provides: "BcbSgsConnector with SERIES_REGISTRY of ~50 Brazilian macro series"
      min_lines: 150
    - path: "src/connectors/fred.py"
      provides: "FredConnector with SERIES_REGISTRY of ~50 US macro series"
      min_lines: 150
    - path: "tests/connectors/test_bcb_sgs.py"
      provides: "Tests for BCB SGS connector with respx HTTP mocking"
      min_lines: 50
    - path: "tests/connectors/test_fred.py"
      provides: "Tests for FRED connector with respx HTTP mocking"
      min_lines: 50
  key_links:
    - from: "src/connectors/bcb_sgs.py"
      to: "src/connectors/base.py"
      via: "BcbSgsConnector(BaseConnector) inheritance"
      pattern: "class BcbSgsConnector\\(BaseConnector\\)"
    - from: "src/connectors/fred.py"
      to: "src/connectors/base.py"
      via: "FredConnector(BaseConnector) inheritance"
      pattern: "class FredConnector\\(BaseConnector\\)"
    - from: "src/connectors/bcb_sgs.py"
      to: "src/core/models/macro_series.py"
      via: "_bulk_insert(MacroSeries, records, 'uq_macro_series_natural_key')"
      pattern: "MacroSeries.*uq_macro_series_natural_key"
    - from: "src/connectors/fred.py"
      to: "src/core/models/macro_series.py"
      via: "_bulk_insert(MacroSeries, records, 'uq_macro_series_natural_key')"
      pattern: "MacroSeries.*uq_macro_series_natural_key"
    - from: "src/connectors/fred.py"
      to: "src/core/config.py"
      via: "settings.fred_api_key for FRED API authentication"
      pattern: "settings\\.fred_api_key"

user_setup:
  - service: FRED
    why: "FRED API requires a free API key for authentication"
    env_vars:
      - name: FRED_API_KEY
        source: "Register at https://fred.stlouisfed.org/docs/api/api_key.html, then copy key to .env"
---

<objective>
BCB SGS and FRED macro data connectors with tests.

Purpose: Implement the two primary macro data connectors that write to the macro_series table. BCB SGS provides ~50 Brazilian economic series (inflation, activity, monetary, external, fiscal). FRED provides ~50 US macro series (CPI, PCE, NFP, rates, credit, fiscal). Together these prove the BaseConnector pattern with two real APIs having different formats, date conventions, and quirks.

Output: Two connector modules (bcb_sgs.py, fred.py) with complete series registries, respx-mocked tests, and updated connectors __init__.py.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/02-connectors/02-RESEARCH.md
@.planning/phases/02-connectors/02-01-SUMMARY.md

@src/connectors/base.py
@src/core/utils/parsing.py
@src/core/models/macro_series.py
@src/core/models/series_metadata.py
@src/core/config.py
@src/core/database.py
@tests/conftest.py
@tests/connectors/conftest.py
@tests/fixtures/bcb_sgs_sample.json
@tests/fixtures/fred_sample.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: BCB SGS connector with ~50 series registry, date chunking, and tests</name>
  <files>
    src/connectors/bcb_sgs.py
    tests/connectors/test_bcb_sgs.py
  </files>
  <action>
**Create src/connectors/bcb_sgs.py:**

Class `BcbSgsConnector(BaseConnector)`:

Class-level constants:
- `SOURCE_NAME = "BCB_SGS"`
- `BASE_URL = "https://api.bcb.gov.br"`
- `RATE_LIMIT_PER_SECOND = 3.0` (conservative; BCB allows ~200 req/min but be polite)
- `MAX_DATE_RANGE_YEARS = 10` (BCB rejects queries >10 years since March 2025)

SERIES_REGISTRY dict -- use the EXACT registry from the Fase0 guide (Etapa 4). This is critical. The keys are our internal series IDs and values are BCB SGS numeric codes:

```python
SERIES_REGISTRY = {
    # INFLATION
    "BR_IPCA_MOM": 433,
    "BR_IPCA_YOY": 13522,
    "BR_IPCA_CORE_EX0": 11426,
    "BR_IPCA_CORE_EX3": 27838,
    "BR_IPCA_CORE_MA": 11427,
    "BR_IPCA_CORE_DP": 27839,
    "BR_IPCA_CORE_P55": 4466,
    "BR_IPCA_DIFFUSION": 21379,
    "BR_IPCA15_MOM": 7478,
    "BR_INPC_MOM": 188,
    "BR_IGP_M_MOM": 189,
    "BR_IGP_DI_MOM": 190,
    "BR_IPA_M_MOM": 225,
    "BR_IPC_S_WEEKLY": 7446,
    "BR_IPC_FIPE_WEEKLY": 10764,
    # ACTIVITY
    "BR_GDP_QOQ": 22099,
    "BR_IBC_BR": 24364,
    "BR_INDUSTRIAL_PROD": 21859,
    "BR_RETAIL_CORE": 1455,
    "BR_RETAIL_BROAD": 28473,
    "BR_SERVICES_REV": 23987,
    "BR_CONSUMER_CONF": 4393,
    "BR_BUSINESS_CONF": 7343,
    "BR_CAPACITY_UTIL": 1344,
    "BR_CAGED_NET": 28763,
    "BR_UNEMPLOYMENT": 24369,
    # MONETARY & CREDIT
    "BR_SELIC_TARGET": 432,
    "BR_SELIC_DAILY": 11,
    "BR_CDI_DAILY": 12,
    "BR_CREDIT_GDP": 20539,
    "BR_DEFAULT_PF": 21082,
    "BR_DEFAULT_PJ": 21083,
    "BR_AVG_LENDING": 20714,
    "BR_M1": 1824,
    "BR_M2": 1837,
    "BR_M3": 1838,
    "BR_M4": 1839,
    "BR_MONETARY_BASE": 1788,
    # EXTERNAL
    "BR_TRADE_BALANCE": 22707,
    "BR_CURRENT_ACCOUNT": 22885,
    "BR_CA_GDP": 22918,
    "BR_FDI": 22886,
    "BR_PORT_EQUITY": 22888,
    "BR_PORT_DEBT": 22889,
    "BR_RESERVES": 13621,
    "BR_PTAX_BUY": 1,
    "BR_PTAX_SELL": 10813,
    # FISCAL
    "BR_PRIMARY_BALANCE": 5793,
    "BR_NOMINAL_DEFICIT": 5727,
    "BR_NET_DEBT_GDP": 4513,
    "BR_GROSS_DEBT_GDP": 13762,
}
```

Methods:

`_chunk_date_range(self, start_date: date, end_date: date) -> list[tuple[date, date]]`:
- Split into chunks of MAX_DATE_RANGE_YEARS years each
- Return list of (chunk_start, chunk_end) tuples

`async def fetch_series(self, series_code: int, start_date: date, end_date: date) -> list[dict]`:
- For each chunk from `_chunk_date_range()`:
  - URL: `/dados/serie/bcdata.sgs.{series_code}/dados`
  - Params: `formato=json`, `dataInicial=start.strftime("%d/%m/%Y")`, `dataFinal=end.strftime("%d/%m/%Y")`
  - Call `self._request("GET", url, params=params)`
  - Parse response JSON: each item has `"data"` (DD/MM/YYYY) and `"valor"` (string, period decimal in JSON)
  - Use `parse_numeric_value(item["valor"], ".")` from src.core.utils.parsing
  - Skip records where value is None (empty, "-", etc.)
  - For each valid record, build dict: `{"series_id": <will be set by caller>, "observation_date": parsed_date, "value": parsed_value, "release_time": datetime.now(tz=ZoneInfo("America/Sao_Paulo")), "revision_number": 0, "source": "BCB_SGS"}`
- Return all records from all chunks

`async def fetch(self, start_date: date, end_date: date, series_ids: list[str] | None = None, **kwargs) -> list[dict]`:
- If `series_ids` is None, use all keys from SERIES_REGISTRY
- For each series_id in the list:
  - Get BCB code from SERIES_REGISTRY
  - Call `fetch_series(code, start_date, end_date)`
  - Attach `series_id` info to each record (see IMPORTANT note below)
  - Sleep `1.0 / self.RATE_LIMIT_PER_SECOND` seconds between series for rate limiting
  - Log progress: `"fetching_series"` with series_id, progress (i/total)
  - On error: log warning with series_id and exception, skip, continue
- Return all records

IMPORTANT: The MacroSeries model has `series_id` as FK to `series_metadata.id` (an integer). The connectors should NOT attempt to look up series_metadata IDs during fetch -- that couples connector to seeded metadata. Instead, the `fetch()` method should return records with a `_series_key` field (our internal string ID like "BR_IPCA_MOM") and the `store()` method can either:
  (a) Look up series_metadata by source+code, OR
  (b) Skip the FK entirely by storing the series_id as a separate "source" field

The cleanest approach for Phase 2: since the series_metadata table may not be seeded yet, store with `series_id` set by a lookup helper that does an upsert. BUT the simplest approach that matches the model: provide a method `_resolve_series_id(self, series_key: str) -> int` that queries series_metadata. If not found, auto-creates a minimal row.

REVISED APPROACH (simpler): Since the MacroSeries model requires `series_id` as FK to series_metadata.id, and series metadata is seeded in Phase 4, the connector needs a way to handle this. The cleanest solution: add a helper method `_ensure_series_metadata(series_key: str, bcb_code: int) -> int` that does INSERT ... ON CONFLICT DO NOTHING on series_metadata and returns the id. This requires a data_source record for "BCB_SGS" to exist. Add a `_ensure_data_source()` method that creates the data source row if not present.

Actually, re-reading the model more carefully: `series_id` is `ForeignKey("series_metadata.id")` with type `Mapped[int]`. This means we MUST have series_metadata rows. The connector should:
1. On `store()`, first ensure data_source "BCB_SGS" exists (upsert)
2. Then ensure series_metadata rows exist for each series_key (upsert)
3. Then bulk insert macro_series records with the resolved integer series_id

Implement `_ensure_data_source(self) -> int` and `_ensure_series_metadata(self, series_key: str, bcb_code: int, source_id: int) -> int` as private async helpers. Both use pg_insert with on_conflict_do_nothing and return the id via a SELECT.

`async def store(self, records: list[dict]) -> int`:
- Group records by `_series_key`
- Ensure data_source for BCB_SGS exists, get source_id
- For each series_key, ensure series_metadata exists, get series_id int
- Replace `_series_key` with actual integer `series_id` in each record
- Call `self._bulk_insert(MacroSeries, records, "uq_macro_series_natural_key")`
- Return total inserted count

`async def run(self, start_date, end_date, series_ids=None, **kwargs) -> int`:
- Override to pass series_ids through to fetch

**Create tests/connectors/test_bcb_sgs.py:**

Using respx to mock the BCB API:

- `test_fetch_series_parses_dates_and_values`: Mock `/dados/serie/bcdata.sgs.433/dados` with sample JSON `[{"data":"02/01/2025","valor":"0.16"},{"data":"03/01/2025","valor":"1.31"}]`. Verify connector parses date as 2025-01-02, value as 0.16. Verify connector parses date as 2025-01-03, value as 1.31.

- `test_fetch_series_skips_empty_values`: Mock with `[{"data":"01/01/2025","valor":""},{"data":"02/01/2025","valor":"-"},{"data":"03/01/2025","valor":"0.16"}]`. Verify only 1 record returned (the valid one).

- `test_date_chunking_splits_long_ranges`: Call `_chunk_date_range(date(2010,1,1), date(2025,12,31))` and verify it produces 2 chunks (2010-2019 and 2020-2025, approximately).

- `test_series_registry_has_expected_count`: Assert `len(BcbSgsConnector.SERIES_REGISTRY) >= 48` (the ~50 series from the guide).

- `test_series_registry_has_key_series`: Assert "BR_IPCA_MOM", "BR_SELIC_TARGET", "BR_RESERVES" are in SERIES_REGISTRY.

For all respx tests: create the connector with `async with BcbSgsConnector() as conn:` pattern, using respx.mock context. Do NOT test actual database operations (those need a live DB) -- only test fetch/parse logic. To test fetch without database, test `fetch_series` directly rather than the full `fetch()` pipeline.
  </action>
  <verify>
Run: `python -c "from src.connectors.bcb_sgs import BcbSgsConnector; print(f'BCB SGS registry: {len(BcbSgsConnector.SERIES_REGISTRY)} series')"` -- should print ~50 series.

Run: `python -m pytest tests/connectors/test_bcb_sgs.py -v` -- all tests pass.
  </verify>
  <done>
BcbSgsConnector exists with ~50 series registry matching Fase0 guide, DD/MM/YYYY date parsing, period-decimal value parsing with defensive handling of empty/missing values, 10-year date range chunking, release_time populated with current Sao Paulo time, and ON CONFLICT DO NOTHING idempotent writes. Tests verify parsing, chunking, and registry completeness.
  </done>
</task>

<task type="auto">
  <name>Task 2: FRED connector with ~50 series registry, revision tracking, and tests</name>
  <files>
    src/connectors/fred.py
    tests/connectors/test_fred.py
    src/connectors/__init__.py
  </files>
  <action>
**Create src/connectors/fred.py:**

Class `FredConnector(BaseConnector)`:

Class-level constants:
- `SOURCE_NAME = "FRED"`
- `BASE_URL = "https://api.stlouisfed.org"`
- `RATE_LIMIT_PER_SECOND = 2.0` (FRED allows 120 req/min; be conservative)
- `TIMEOUT_SECONDS = 60.0` (FRED can be slow for large series)

SERIES_REGISTRY dict -- use the EXACT registry from the Fase0 guide (Etapa 5). Keys are our internal series IDs, values are FRED series codes:

```python
SERIES_REGISTRY = {
    # INFLATION
    "US_CPI_ALL_SA": "CPIAUCSL",
    "US_CPI_ALL_NSA": "CPIAUCNS",
    "US_CPI_CORE": "CPILFESL",
    "US_CPI_TRIMMED": "TRMMEANCPIM158SFRBCLE",
    "US_CPI_MEDIAN": "MEDCPIM158SFRBCLE",
    "US_CPI_STICKY": "STICKCPIM157SFRBATL",
    "US_CPI_FLEXIBLE": "FLEXCPIM157SFRBATL",
    "US_PCE_HEADLINE": "PCEPI",
    "US_PCE_CORE": "PCEPILFE",
    "US_PPI_ALL": "PPIACO",
    "US_MICHIGAN_INF_1Y": "MICH",
    "US_BEI_5Y": "T5YIE",
    "US_BEI_10Y": "T10YIE",
    "US_FWD_INF_5Y5Y": "T5YIFR",
    # ACTIVITY & LABOR
    "US_GDP_REAL": "GDPC1",
    "US_NFP_TOTAL": "PAYEMS",
    "US_NFP_PRIVATE": "USPRIV",
    "US_UNEMP_U3": "UNRATE",
    "US_UNEMP_U6": "U6RATE",
    "US_AVG_HOURLY_EARN": "CES0500000003",
    "US_JOLTS_OPENINGS": "JTSJOL",
    "US_JOLTS_QUITS": "JTSQUR",
    "US_INITIAL_CLAIMS": "ICSA",
    "US_CONT_CLAIMS": "CCSA",
    "US_INDPRO": "INDPRO",
    "US_CAP_UTIL": "TCU",
    "US_RETAIL_TOTAL": "RSAFS",
    "US_RETAIL_CONTROL": "RSFSXMV",
    "US_HOUSING_STARTS": "HOUST",
    "US_BUILDING_PERMITS": "PERMIT",
    "US_PERSONAL_INCOME": "PI",
    "US_PERSONAL_SPENDING": "PCE",
    "US_CONSUMER_SENT": "UMCSENT",
    "US_CFNAI": "CFNAI",
    # MONETARY & RATES
    "US_FED_FUNDS": "DFF",
    "US_SOFR": "SOFR",
    "US_UST_2Y": "DGS2",
    "US_UST_5Y": "DGS5",
    "US_UST_10Y": "DGS10",
    "US_UST_30Y": "DGS30",
    "US_TIPS_5Y": "DFII5",
    "US_TIPS_10Y": "DFII10",
    "US_FED_TOTAL_ASSETS": "WALCL",
    "US_FED_TREASURIES": "WTREGEN",
    "US_FED_MBS": "WSHOMCB",
    "US_ON_RRP": "RRPONTSYD",
    "US_NFCI": "NFCI",
    # CREDIT
    "US_HY_OAS": "BAMLH0A0HYM2",
    "US_IG_OAS": "BAMLC0A0CM",
    # FISCAL
    "US_FED_DEBT": "GFDEBTN",
    "US_DEBT_GDP": "GFDEGDQ188S",
}
```

Also define `REVISABLE_SERIES`: a set of series IDs known to be revised (GDP, NFP, PCE, etc.):
```python
REVISABLE_SERIES = {"US_GDP_REAL", "US_NFP_TOTAL", "US_NFP_PRIVATE", "US_PCE_HEADLINE", "US_PCE_CORE", "US_PERSONAL_INCOME", "US_PERSONAL_SPENDING", "US_INDPRO", "US_RETAIL_TOTAL", "US_RETAIL_CONTROL"}
```

Methods:

`async def fetch_series(self, series_code: str, start_date: date, end_date: date, realtime_start: date | None = None, realtime_end: date | None = None) -> list[dict]`:
- URL: `/fred/series/observations`
- Params: `series_id=series_code`, `api_key=settings.fred_api_key`, `file_type=json`, `observation_start=start_date.strftime("%Y-%m-%d")`, `observation_end=end_date.strftime("%Y-%m-%d")`
- If realtime_start/end provided, add them to params (for vintage/revision tracking)
- Call `self._request("GET", url, params=params)`
- Parse response: `data.get("observations", [])`
- For each observation:
  - Skip if `obs["value"] == "."` (FRED missing value convention)
  - Parse date from `obs["date"]` (YYYY-MM-DD format)
  - Parse value via float()
  - Set `release_time` from `obs["realtime_start"]` with ZoneInfo("America/New_York")
  - Set `revision_number = 0` (for Phase 2; Phase 4 backfill may use vintages for revisable series)
- Return list of record dicts

`async def fetch(self, start_date: date, end_date: date, series_ids: list[str] | None = None, **kwargs) -> list[dict]`:
- Similar pattern to BCB SGS: iterate over series, fetch each, rate-limit, log progress, skip on error
- Attach `_series_key` to each record
- Return all records

`async def store(self, records: list[dict]) -> int`:
- Same pattern as BCB SGS: ensure data_source "FRED", ensure series_metadata rows, resolve integer series_ids, bulk insert with ON CONFLICT DO NOTHING
- The `_ensure_data_source` and `_ensure_series_metadata` pattern should be in BaseConnector (or duplicated here; keeping it DRY is better). If these helpers were added to BaseConnector in Plan 01, use them. If not, add them to each connector -- or better, add a shared mixin/helper in base.py.

Actually, refactor: add `_ensure_data_source(self, name, base_url, auth_type, rate_limit) -> int` and `_ensure_series_metadata(self, series_key, source_id, code, name, frequency, country, unit) -> int` as methods on BaseConnector in base.py. These use the same pg_insert...on_conflict_do_nothing pattern. This means modifying base.py -- add it to the files list for this task.

If base.py was not updated in Plan 01 with these helpers, update it now. These are generic enough to belong there.

`async def fetch_vintages(self, series_code: str, observation_date: date) -> list[dict]`:
- For future use (Phase 4): fetch all vintages of a specific observation using realtime_start/end sweeping
- For now, implement as a stub that fetches with full realtime range and assigns incrementing revision_number
- This proves the DATA-02 revision tracking pattern

**Update src/connectors/__init__.py:**
- Add imports: `from .bcb_sgs import BcbSgsConnector`, `from .fred import FredConnector`

**Create tests/connectors/test_fred.py:**

Using respx to mock the FRED API:

- `test_fetch_series_parses_observations`: Mock `/fred/series/observations` with sample FRED JSON containing 3 observations. Verify dates and values parsed correctly.

- `test_fetch_series_skips_missing_values`: Mock with observations including `"value": "."`. Verify missing values are skipped.

- `test_fetch_series_includes_realtime_start_as_release_time`: Verify the `release_time` field is set from `realtime_start` in the response.

- `test_series_registry_has_expected_count`: Assert `len(FredConnector.SERIES_REGISTRY) >= 48`.

- `test_series_registry_has_key_series`: Assert "US_CPI_ALL_SA", "US_FED_FUNDS", "US_UST_10Y", "US_NFP_TOTAL" are present.

- `test_fetch_requires_api_key`: Verify the request includes api_key parameter.

For all respx tests: use `respx.mock(base_url="https://api.stlouisfed.org")` pattern. Test fetch_series directly without database.
  </action>
  <verify>
Run: `python -c "from src.connectors.fred import FredConnector; print(f'FRED registry: {len(FredConnector.SERIES_REGISTRY)} series')"` -- should print ~50 series.

Run: `python -m pytest tests/connectors/test_bcb_sgs.py tests/connectors/test_fred.py -v` -- all tests pass.

Run: `python -c "from src.connectors import BcbSgsConnector, FredConnector; print('Both connectors importable')"` -- should succeed.
  </verify>
  <done>
BCB SGS connector with ~50 series from Fase0 guide, DD/MM/YYYY parsing, 10-year chunking, release_time=now(Sao_Paulo), revision_number=0. FRED connector with ~50 series from Fase0 guide, missing-value ("." skip), release_time=realtime_start, revision tracking pattern via revision_number and REVISABLE_SERIES set. Both write to macro_series via ON CONFLICT DO NOTHING. All tests pass with respx mocking.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.connectors.bcb_sgs import BcbSgsConnector"` imports successfully
2. `python -c "from src.connectors.fred import FredConnector"` imports successfully
3. `len(BcbSgsConnector.SERIES_REGISTRY) >= 48` (all series from Fase0 guide)
4. `len(FredConnector.SERIES_REGISTRY) >= 48` (all series from Fase0 guide)
5. `python -m pytest tests/connectors/test_bcb_sgs.py tests/connectors/test_fred.py -v` -- all pass
6. BCB SGS correctly parses DD/MM/YYYY dates in tests
7. FRED correctly skips "." missing values in tests
8. Both connectors reference MacroSeries model and "uq_macro_series_natural_key" constraint
</verification>

<success_criteria>
- BCB SGS connector fetches Brazilian macro series with correct comma-decimal parsing and DD/MM/YYYY date handling
- BCB SGS connector has 10-year date range chunking to comply with API limits
- FRED connector fetches US macro series with missing-value handling (value "." skipped)
- FRED connector stores release_time from realtime_start for point-in-time correctness
- Both connectors use ON CONFLICT DO NOTHING for idempotent writes
- Both SERIES_REGISTRY dicts match the exact codes from the Fase0 guide
- All tests pass with respx HTTP mocking
</success_criteria>

<output>
After completion, create `.planning/phases/02-connectors/02-02-SUMMARY.md`
</output>
