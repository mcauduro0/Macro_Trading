---
phase: 01-foundation
plan: 02
type: execute
wave: 2
depends_on:
  - 01-01
files_modified:
  - src/core/models/__init__.py
  - src/core/models/base.py
  - src/core/models/instruments.py
  - src/core/models/series_metadata.py
  - src/core/models/data_sources.py
  - src/core/models/market_data.py
  - src/core/models/macro_series.py
  - src/core/models/curves.py
  - src/core/models/flow_data.py
  - src/core/models/fiscal_data.py
  - src/core/models/vol_surfaces.py
  - src/core/models/signals.py
  - alembic.ini
  - alembic/env.py
  - alembic/script.py.mako
  - alembic/versions/001_initial_schema.py
autonomous: true
requirements:
  - INFRA-02
  - INFRA-03
  - INFRA-04

must_haves:
  truths:
    - "Running alembic upgrade head creates all 10 tables in TimescaleDB with the TimescaleDB extension enabled"
    - "7 tables are registered as hypertables (market_data, macro_series, curves, flow_data, fiscal_data, vol_surfaces, signals)"
    - "All 7 hypertables have compression policies configured with correct segmentby columns"
    - "All hypertable models use composite primary keys that include the time partitioning column"
    - "Every hypertable has a natural-key UniqueConstraint for ON CONFLICT DO NOTHING idempotent writes"
    - "All timestamp/datetime columns use TIMESTAMPTZ (timezone=True), never naive TIMESTAMP"
  artifacts:
    - path: "src/core/models/base.py"
      provides: "DeclarativeBase with naming conventions"
      contains: "class Base"
    - path: "src/core/models/instruments.py"
      provides: "Instrument metadata table"
      contains: "class Instrument"
    - path: "src/core/models/series_metadata.py"
      provides: "Series metadata table"
      contains: "class SeriesMetadata"
    - path: "src/core/models/data_sources.py"
      provides: "Data source registry table"
      contains: "class DataSource"
    - path: "src/core/models/market_data.py"
      provides: "Market data hypertable model"
      contains: "class MarketData"
    - path: "src/core/models/macro_series.py"
      provides: "Macro series hypertable with release_time for PIT correctness"
      contains: "class MacroSeries"
    - path: "src/core/models/curves.py"
      provides: "Curves hypertable model"
      contains: "class CurveData"
    - path: "src/core/models/flow_data.py"
      provides: "Flow data hypertable model"
      contains: "class FlowData"
    - path: "src/core/models/fiscal_data.py"
      provides: "Fiscal data hypertable model"
      contains: "class FiscalData"
    - path: "src/core/models/vol_surfaces.py"
      provides: "Vol surface hypertable model"
      contains: "class VolSurface"
    - path: "src/core/models/signals.py"
      provides: "Signals hypertable model"
      contains: "class Signal"
    - path: "src/core/models/__init__.py"
      provides: "Re-exports Base and all 10 model classes"
      contains: "from .base import Base"
    - path: "alembic/env.py"
      provides: "Alembic env configured to filter TimescaleDB indexes and schemas"
      contains: "include_name"
    - path: "alembic/versions/001_initial_schema.py"
      provides: "Initial migration with hypertable creation and compression"
      contains: "create_hypertable"
  key_links:
    - from: "alembic/env.py"
      to: "src/core/models/__init__.py"
      via: "Imports all models so they register with Base.metadata"
      pattern: "from src.core.models"
    - from: "alembic.ini"
      to: "src/core/config.py"
      via: "Uses sync_database_url (psycopg2) for migration connection"
      pattern: "postgresql\\+psycopg2"
    - from: "alembic/versions/001_initial_schema.py"
      to: "TimescaleDB"
      via: "Raw SQL for create_hypertable and compression policies"
      pattern: "create_hypertable|add_compression_policy"
    - from: "src/core/models/macro_series.py"
      to: "src/core/models/series_metadata.py"
      via: "ForeignKey from macro_series.series_id to series_metadata.id"
      pattern: "ForeignKey.*series_metadata"
---

<objective>
Create all 10 SQLAlchemy 2.0 ORM models and the Alembic migration that creates the database schema with TimescaleDB hypertables and compression policies.

Purpose: Define the complete database schema that all connectors (Phase 2-3), transforms (Phase 5), and API (Phase 6) will write to and read from. The schema must support point-in-time correctness via release_time, idempotent writes via natural-key unique constraints, and efficient time-series queries via TimescaleDB hypertables with compression.

Output: 10 model files (3 metadata + 7 hypertable), Alembic configuration, and an initial migration that creates all tables, converts 7 to hypertables, and configures compression policies.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-CONTEXT.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/research/ARCHITECTURE.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create all 10 SQLAlchemy 2.0 ORM models</name>
  <files>
    src/core/models/__init__.py
    src/core/models/base.py
    src/core/models/instruments.py
    src/core/models/series_metadata.py
    src/core/models/data_sources.py
    src/core/models/market_data.py
    src/core/models/macro_series.py
    src/core/models/curves.py
    src/core/models/flow_data.py
    src/core/models/fiscal_data.py
    src/core/models/vol_surfaces.py
    src/core/models/signals.py
  </files>
  <action>
    Create `src/core/models/` package with all 10 models following SQLAlchemy 2.0 patterns from RESEARCH.md.

    **base.py** -- DeclarativeBase with naming conventions:
    - Define `convention` dict for ix, uq, ck, fk, pk naming patterns (exactly as in RESEARCH.md)
    - `class Base(DeclarativeBase)` with `metadata = MetaData(naming_convention=convention)`

    **3 Metadata Tables (regular PostgreSQL, auto-increment PK):**

    **instruments.py** -- `class Instrument(Base)`:
    - `__tablename__ = "instruments"`
    - Columns: id (int, PK, autoincrement), ticker (String(50), unique, not null), name (String(200), not null), asset_class (String(50), not null), country (String(10), not null), currency (String(10), not null), exchange (String(50), nullable), is_active (bool, default True), created_at (DateTime(timezone=True), server_default=func.now()), updated_at (DateTime(timezone=True), server_default=func.now(), onupdate=func.now())

    **data_sources.py** -- `class DataSource(Base)`:
    - `__tablename__ = "data_sources"`
    - Columns: id (int, PK, autoincrement), name (String(100), unique, not null), base_url (String(500), not null), auth_type (String(50), not null, e.g. "none", "api_key", "oauth"), rate_limit_per_minute (int, nullable), default_locale (String(10), default "en-US"), notes (Text, nullable), is_active (bool, default True), created_at (DateTime(timezone=True), server_default=func.now())

    **series_metadata.py** -- `class SeriesMetadata(Base)`:
    - `__tablename__ = "series_metadata"`
    - Columns: id (int, PK, autoincrement), source_id (int, ForeignKey("data_sources.id"), not null), series_code (String(100), not null), name (String(300), not null), description (Text, nullable), frequency (String(20), not null), country (String(10), not null), unit (String(50), not null), decimal_separator (String(5), default "."), date_format (String(20), default "YYYY-MM-DD"), is_revisable (bool, default False), release_lag_days (int, nullable), release_timezone (String(50), default "UTC"), is_active (bool, default True), created_at (DateTime(timezone=True), server_default=func.now()), updated_at (DateTime(timezone=True), server_default=func.now(), onupdate=func.now())
    - UniqueConstraint on (source_id, series_code) -- each source has unique series codes

    **7 Hypertable Models (composite PK including time column):**

    ALL hypertable models MUST:
    - Include the time partitioning column in the primary key (composite PK)
    - Use `DateTime(timezone=True)` for all datetime columns (TIMESTAMPTZ), or `Date` for date-only columns
    - Have a natural-key UniqueConstraint for ON CONFLICT DO NOTHING
    - Have an Index on the series/instrument ID column for fast lookups

    **market_data.py** -- `class MarketData(Base)`:
    - `__tablename__ = "market_data"`
    - PK: (id BigInteger autoincrement, timestamp DateTime(timezone=True))
    - Columns: instrument_id (int, FK instruments.id, not null), timestamp (TIMESTAMPTZ, PK, not null), frequency (String(20), not null, default "daily"), open (Float, nullable), high (Float, nullable), low (Float, nullable), close (Float, nullable), volume (Float, nullable), adjusted_close (Float, nullable), source (String(50), nullable)
    - UniqueConstraint: (instrument_id, timestamp, frequency) named "uq_market_data_natural_key"
    - Index on instrument_id

    **macro_series.py** -- `class MacroSeries(Base)`:
    - `__tablename__ = "macro_series"`
    - PK: (id BigInteger autoincrement, observation_date Date)
    - Columns: series_id (int, FK series_metadata.id, not null), observation_date (Date, PK, not null), value (Float, not null), release_time (DateTime(timezone=True), not null, comment="When this value became known"), revision_number (SmallInteger, default 0), source (String(50), nullable)
    - UniqueConstraint: (series_id, observation_date, revision_number) named "uq_macro_series_natural_key"
    - Index on series_id

    **curves.py** -- `class CurveData(Base)`:
    - `__tablename__ = "curves"`
    - PK: (id BigInteger autoincrement, curve_date Date)
    - Columns: curve_id (String(50), not null -- identifies the curve, e.g. "DI_PRE", "UST_NOMINAL"), curve_date (Date, PK, not null), tenor_days (int, not null), tenor_label (String(20), not null, e.g. "1Y", "5Y", "10Y"), rate (Float, not null), curve_type (String(20), not null), source (String(50), nullable)
    - UniqueConstraint: (curve_id, curve_date, tenor_days) named "uq_curves_natural_key"
    - Index on curve_id

    **flow_data.py** -- `class FlowData(Base)`:
    - `__tablename__ = "flow_data"`
    - PK: (id BigInteger autoincrement, observation_date Date)
    - Columns: series_id (int, FK series_metadata.id, not null), observation_date (Date, PK, not null), value (Float, not null), flow_type (String(50), not null), unit (String(20), not null, default "USD_MM"), release_time (DateTime(timezone=True), nullable)
    - UniqueConstraint: (series_id, observation_date, flow_type) named "uq_flow_data_natural_key"
    - Index on series_id

    **fiscal_data.py** -- `class FiscalData(Base)`:
    - `__tablename__ = "fiscal_data"`
    - PK: (id BigInteger autoincrement, observation_date Date)
    - Columns: series_id (int, FK series_metadata.id, not null), observation_date (Date, PK, not null), value (Float, not null), fiscal_metric (String(50), not null), unit (String(20), not null, default "BRL_MM"), release_time (DateTime(timezone=True), nullable)
    - UniqueConstraint: (series_id, observation_date, fiscal_metric) named "uq_fiscal_data_natural_key"
    - Index on series_id

    **vol_surfaces.py** -- `class VolSurface(Base)`:
    - `__tablename__ = "vol_surfaces"`
    - PK: (id BigInteger autoincrement, surface_date Date)
    - Columns: instrument_id (int, FK instruments.id, not null), surface_date (Date, PK, not null), delta (Float, not null), tenor_days (int, not null), implied_vol (Float, not null), call_put (String(4), nullable, e.g. "call", "put"), source (String(50), nullable)
    - UniqueConstraint: (instrument_id, surface_date, delta, tenor_days) named "uq_vol_surfaces_natural_key"
    - Index on instrument_id

    **signals.py** -- `class Signal(Base)`:
    - `__tablename__ = "signals"`
    - PK: (id BigInteger autoincrement, signal_date Date)
    - Columns: signal_type (String(50), not null), signal_date (Date, PK, not null), instrument_id (int, FK instruments.id, nullable), series_id (int, FK series_metadata.id, nullable), value (Float, not null), confidence (Float, nullable), metadata_json (Text, nullable, for JSON blob)
    - UniqueConstraint: (signal_type, signal_date, instrument_id) named "uq_signals_natural_key"
    - Index on signal_type

    **__init__.py** -- Re-export Base and all 10 model classes:
    ```python
    from .base import Base
    from .instruments import Instrument
    from .series_metadata import SeriesMetadata
    from .data_sources import DataSource
    from .market_data import MarketData
    from .macro_series import MacroSeries
    from .curves import CurveData
    from .flow_data import FlowData
    from .fiscal_data import FiscalData
    from .vol_surfaces import VolSurface
    from .signals import Signal

    __all__ = [
        "Base", "Instrument", "SeriesMetadata", "DataSource",
        "MarketData", "MacroSeries", "CurveData", "FlowData",
        "FiscalData", "VolSurface", "Signal",
    ]
    ```
  </action>
  <verify>
    Run: `python -c "from src.core.models import Base, Instrument, MacroSeries, MarketData, CurveData, FlowData, FiscalData, VolSurface, Signal, SeriesMetadata, DataSource; print(f'{len(Base.metadata.tables)} tables registered'); assert len(Base.metadata.tables) == 10"`
    Command succeeds and prints "10 tables registered".
  </verify>
  <done>
    All 10 SQLAlchemy 2.0 model classes exist, import cleanly, and register 10 tables with Base.metadata. All 7 hypertable models use composite primary keys including the time column. All datetime columns use timezone=True (TIMESTAMPTZ). Every hypertable has a natural-key UniqueConstraint for idempotent writes. MacroSeries has release_time (TIMESTAMPTZ) for point-in-time correctness.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create Alembic configuration and initial migration with hypertables and compression</name>
  <files>
    alembic.ini
    alembic/env.py
    alembic/script.py.mako
    alembic/versions/001_initial_schema.py
  </files>
  <action>
    **Initialize Alembic** -- Do NOT run `alembic init` because we need custom env.py. Create files manually.

    **alembic.ini:**
    - Set `script_location = alembic`
    - Set `sqlalchemy.url = postgresql+psycopg2://macro_user:macro_pass@localhost:5432/macro_trading`
    - Standard logging config (console handler, alembic logger at INFO, sqlalchemy.engine at WARN)
    - Note: This uses psycopg2 (sync driver) because Alembic CLI is synchronous. The async driver (asyncpg) is for application code only.

    **alembic/env.py** -- Follow RESEARCH.md Pattern 5 exactly:
    - Import `Base` from `src.core.models.base`
    - Import ALL model modules from `src.core.models` (noqa: F401) so they register with metadata
    - Set `target_metadata = Base.metadata`
    - Define `TIMESCALEDB_SCHEMAS` set with all 6 internal schema names
    - Define `KNOWN_HYPERTABLE_TIME_COLS` dict mapping table names to their time columns
    - Implement `_is_timescaledb_index(name)` function
    - Implement `include_name(name, type_, parent_names)` filter function
    - Implement `run_migrations_offline()` and `run_migrations_online()` both using `include_name`
    - Use `pool.NullPool` for online migrations (Alembic best practice)

    **alembic/script.py.mako** -- Standard Alembic template (default from alembic init).

    **alembic/versions/001_initial_schema.py** -- Manual migration (NOT autogenerated):
    - Revision ID: generate a random hex string (e.g., "a1b2c3d4e5f6")
    - revision: use the hex string
    - down_revision: None

    `upgrade()` function, in this exact order:
    1. Enable TimescaleDB extension: `CREATE EXTENSION IF NOT EXISTS timescaledb CASCADE;`
    2. Create all 10 tables using `op.create_table()` for each (include all columns, constraints, indexes as defined in models). Create metadata tables FIRST (data_sources, instruments, series_metadata) because hypertables have FKs to them.
    3. Convert 7 tables to hypertables using `op.execute()` with `SELECT create_hypertable(...)`:
       - market_data on 'timestamp', chunk_time_interval INTERVAL '1 month'
       - macro_series on 'observation_date', chunk_time_interval INTERVAL '1 year'
       - curves on 'curve_date', chunk_time_interval INTERVAL '3 months'
       - flow_data on 'observation_date', chunk_time_interval INTERVAL '1 year'
       - fiscal_data on 'observation_date', chunk_time_interval INTERVAL '1 year'
       - vol_surfaces on 'surface_date', chunk_time_interval INTERVAL '1 year'
       - signals on 'signal_date', chunk_time_interval INTERVAL '1 year'
       - All with if_not_exists => TRUE
    4. Enable compression on all 7 hypertables using `ALTER TABLE ... SET (timescaledb.compress, timescaledb.compress_segmentby, timescaledb.compress_orderby)`:
       - market_data: segmentby='instrument_id', orderby='timestamp DESC'
       - macro_series: segmentby='series_id', orderby='observation_date DESC'
       - curves: segmentby='curve_id', orderby='curve_date DESC'
       - flow_data: segmentby='series_id', orderby='observation_date DESC'
       - fiscal_data: segmentby='series_id', orderby='observation_date DESC'
       - vol_surfaces: segmentby='instrument_id', orderby='surface_date DESC'
       - signals: segmentby='signal_type', orderby='signal_date DESC'
    5. Add compression policies using `SELECT add_compression_policy(...)`:
       - market_data: 30 days
       - macro_series: 90 days
       - curves: 90 days
       - flow_data: 90 days
       - fiscal_data: 180 days
       - vol_surfaces: 90 days
       - signals: 90 days

    `downgrade()` function:
    1. Remove compression policies (with if_exists => true)
    2. Disable compression on all hypertables
    3. Drop all tables in reverse dependency order (hypertables first, then metadata)
    4. Drop TimescaleDB extension

    **After creating the migration, run it:**
    - Ensure Docker TimescaleDB is running (from Plan 01)
    - Run `alembic upgrade head`
    - Verify: query `SELECT tablename FROM pg_tables WHERE schemaname='public'` shows 10 tables
    - Verify: query `SELECT hypertable_name FROM timescaledb_information.hypertables` shows 7 hypertables
    - Verify: query `SELECT hypertable_name FROM timescaledb_information.compression_settings` shows 7 entries with correct segmentby values
  </action>
  <verify>
    Run: `cd /home/user/Macro_Trading && alembic upgrade head` (migration succeeds without errors)
    Run: `python -c "
import psycopg2
conn = psycopg2.connect('postgresql://macro_user:macro_pass@localhost:5432/macro_trading')
cur = conn.cursor()
cur.execute(\"SELECT tablename FROM pg_tables WHERE schemaname='public' ORDER BY tablename\")
tables = [r[0] for r in cur.fetchall()]
print(f'Tables ({len(tables)}): {tables}')
assert len(tables) >= 10, f'Expected 10 tables, got {len(tables)}'
cur.execute('SELECT hypertable_name FROM timescaledb_information.hypertables ORDER BY hypertable_name')
hypertables = [r[0] for r in cur.fetchall()]
print(f'Hypertables ({len(hypertables)}): {hypertables}')
assert len(hypertables) == 7, f'Expected 7 hypertables, got {len(hypertables)}'
cur.execute('SELECT hypertable_name FROM timescaledb_information.compression_settings')
compressed = list(set(r[0] for r in cur.fetchall()))
print(f'Compression configured on: {compressed}')
assert len(compressed) == 7
conn.close()
print('ALL CHECKS PASSED')
"`
    All assertions pass: 10 tables created, 7 hypertables registered, 7 compression policies active.
  </verify>
  <done>
    Alembic is configured with TimescaleDB-aware env.py (filters internal schemas and auto-created indexes). Running `alembic upgrade head` creates all 10 tables, converts 7 to hypertables with correct chunk intervals, enables compression with proper segmentby columns, and sets compression policies with generous delays. The Alembic version table tracks the migration. Running `alembic revision --autogenerate` would produce an empty migration (no drift).
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.core.models import Base; assert len(Base.metadata.tables) == 10"` -- 10 models registered
2. `alembic upgrade head` completes without errors
3. Database has 10 tables in public schema
4. 7 tables are registered as TimescaleDB hypertables
5. All 7 hypertables have compression policies with correct segmentby
6. All hypertable PKs include the time partitioning column
7. `alembic revision --autogenerate -m "check"` produces empty migration (no schema drift) -- then delete the empty migration
</verification>

<success_criteria>
- 10 SQLAlchemy 2.0 model classes with Mapped[] type hints import cleanly from src.core.models
- 3 metadata tables (instruments, series_metadata, data_sources) use standard auto-increment PKs
- 7 hypertable models use composite PKs including the time column
- All datetime columns are TIMESTAMPTZ (timezone=True)
- Every hypertable has a natural-key UniqueConstraint
- MacroSeries has release_time field for point-in-time correctness
- Alembic migration creates all tables, hypertables, and compression policies in one operation
- Alembic env.py filters TimescaleDB internal schemas and auto-created indexes
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-02-SUMMARY.md`
</output>
