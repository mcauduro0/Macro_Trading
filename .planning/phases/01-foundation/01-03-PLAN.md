---
phase: 01-foundation
plan: 03
type: execute
wave: 2
depends_on:
  - 01-01
files_modified:
  - src/core/database.py
  - src/core/redis.py
  - scripts/verify_connectivity.py
autonomous: true
requirements:
  - INFRA-06
  - INFRA-07

must_haves:
  truths:
    - "Python application connects to TimescaleDB asynchronously via asyncpg and can execute a SELECT query"
    - "Python application connects to TimescaleDB synchronously via psycopg2 for migrations and scripts"
    - "Redis client singleton connects with a connection pool and can SET/GET values"
    - "Async session factory produces sessions with autoflush=False and expire_on_commit=False"
    - "A verification script confirms both database and Redis connectivity end-to-end"
  artifacts:
    - path: "src/core/database.py"
      provides: "Async engine (asyncpg), sync engine (psycopg2), session factories, get_async_session dependency"
      contains: "create_async_engine"
    - path: "src/core/redis.py"
      provides: "Redis async client singleton with ConnectionPool lifecycle management"
      contains: "get_redis"
    - path: "scripts/verify_connectivity.py"
      provides: "Standalone script that verifies DB and Redis connections work"
      contains: "asyncio.run"
  key_links:
    - from: "src/core/database.py"
      to: "src/core/config.py"
      via: "Reads async_database_url and sync_database_url from settings singleton"
      pattern: "settings\\.async_database_url|settings\\.sync_database_url"
    - from: "src/core/redis.py"
      to: "src/core/config.py"
      via: "Reads redis_url and redis_max_connections from settings singleton"
      pattern: "settings\\.redis_url|settings\\.redis_max_connections"
    - from: "scripts/verify_connectivity.py"
      to: "src/core/database.py"
      via: "Imports async_session_factory and async_engine"
      pattern: "from src.core.database import"
    - from: "scripts/verify_connectivity.py"
      to: "src/core/redis.py"
      via: "Imports get_redis and close_redis"
      pattern: "from src.core.redis import"
---

<objective>
Create the database engine layer (async + sync) and Redis client singleton, then verify full connectivity to all services.

Purpose: Provide the data access layer that all application code (connectors, transforms, API) will use. The async engine powers FastAPI request handling and connector ingestion. The sync engine powers Alembic migrations and seed scripts. The Redis client powers caching in the API layer. A verification script proves the entire stack is wired correctly.

Output: database.py with async/sync engines and session factories, redis.py with singleton client and pool management, and a verification script that confirms end-to-end connectivity.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-foundation/01-CONTEXT.md
@.planning/phases/01-foundation/01-RESEARCH.md
@.planning/phases/01-foundation/01-01-SUMMARY.md
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create database engine layer and Redis client singleton</name>
  <files>
    src/core/database.py
    src/core/redis.py
  </files>
  <action>
    **src/core/database.py** -- Follow RESEARCH.md Database Engine Setup pattern exactly:

    Import `create_async_engine`, `async_sessionmaker`, `AsyncSession` from `sqlalchemy.ext.asyncio`.
    Import `create_engine`, `sessionmaker`, `Session` from `sqlalchemy` / `sqlalchemy.orm`.
    Import `settings` from `src.core.config`.
    Import `AsyncGenerator` from `collections.abc`.

    **Async engine** (for application runtime):
    ```python
    async_engine = create_async_engine(
        settings.async_database_url,
        pool_size=settings.db_pool_size,
        max_overflow=settings.db_max_overflow,
        pool_pre_ping=settings.db_pool_pre_ping,
        echo=settings.debug,
    )
    ```

    **Async session factory:**
    ```python
    async_session_factory = async_sessionmaker(
        async_engine,
        class_=AsyncSession,
        autoflush=False,
        expire_on_commit=False,
    )
    ```

    **Sync engine** (for Alembic, seeds, scripts):
    ```python
    sync_engine = create_engine(
        settings.sync_database_url,
        pool_size=5,
        pool_pre_ping=True,
        echo=settings.debug,
    )
    ```

    **Sync session factory:**
    ```python
    sync_session_factory = sessionmaker(
        sync_engine,
        autoflush=False,
        expire_on_commit=False,
    )
    ```

    **Dependency injector** for FastAPI:
    ```python
    async def get_async_session() -> AsyncGenerator[AsyncSession, None]:
        async with async_session_factory() as session:
            try:
                yield session
            except Exception:
                await session.rollback()
                raise
    ```

    **Sync session helper** for scripts:
    ```python
    def get_sync_session() -> Session:
        return sync_session_factory()
    ```

    ---

    **src/core/redis.py** -- Follow RESEARCH.md Redis Client Singleton pattern exactly:

    Import `redis.asyncio as aioredis`.
    Import `settings` from `src.core.config`.

    Module-level globals: `_redis_pool` and `_redis_client` (both initialized to None).

    **get_redis()** async function:
    - If `_redis_client` is None:
      - Create `ConnectionPool.from_url(settings.redis_url, max_connections=settings.redis_max_connections, decode_responses=True)`
      - Create `aioredis.Redis(connection_pool=_redis_pool)` -- use `connection_pool=` parameter, NOT `from_pool()`, to avoid premature pool closure (per Pitfall 5 from RESEARCH.md)
    - Return `_redis_client`

    **close_redis()** async function:
    - If `_redis_client` is not None: `await _redis_client.aclose()`, set to None
    - If `_redis_pool` is not None: `await _redis_pool.aclose()` (or `await _redis_pool.disconnect()`), set to None
    - This function must be called during application shutdown (FastAPI lifespan)

    **Key design notes:**
    - Singleton pattern ensures one connection pool for the entire application
    - `decode_responses=True` means Redis returns strings (not bytes) -- important for caching JSON
    - Pool lifecycle is managed independently from the client (per RESEARCH.md Pitfall 5)
  </action>
  <verify>
    Run: `python -c "from src.core.database import async_engine, sync_engine, async_session_factory, sync_session_factory, get_async_session; print('Database module OK')"` (imports succeed)
    Run: `python -c "from src.core.redis import get_redis, close_redis; print('Redis module OK')"` (imports succeed)
    Both commands succeed without import errors.
  </verify>
  <done>
    database.py provides async_engine (asyncpg, pool_size=20), sync_engine (psycopg2, pool_size=5), async_session_factory (autoflush=False, expire_on_commit=False), sync_session_factory, get_async_session (FastAPI dependency), and get_sync_session helper. redis.py provides get_redis() async singleton with ConnectionPool and close_redis() for shutdown cleanup.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create connectivity verification script</name>
  <files>
    scripts/verify_connectivity.py
  </files>
  <action>
    Create `scripts/verify_connectivity.py` -- a standalone script that verifies the complete infrastructure stack is working.

    The script should:
    1. Print a header banner: "=== Macro Trading: Infrastructure Connectivity Check ==="
    2. **Test TimescaleDB async connection:**
       - Import `async_session_factory` from `src.core.database`
       - Open an async session
       - Execute `SELECT 1` and verify result is 1
       - Execute `SELECT default_version, installed_version FROM pg_available_extensions WHERE name = 'timescaledb'` and print TimescaleDB version
       - Execute `SELECT hypertable_name FROM timescaledb_information.hypertables ORDER BY hypertable_name` and print all hypertable names
       - Execute `SELECT COUNT(*) FROM information_schema.tables WHERE table_schema = 'public'` and print table count
       - Print PASS/FAIL for each check
    3. **Test TimescaleDB sync connection:**
       - Import `get_sync_session` from `src.core.database`
       - Open a sync session, execute `SELECT 1`, verify
       - Print PASS/FAIL
    4. **Test Redis connection:**
       - Import `get_redis`, `close_redis` from `src.core.redis`
       - Get Redis client
       - Execute `PING` and verify response is True (or "PONG")
       - SET a test key `macro:test:connectivity` with value `ok` and TTL 60
       - GET the key back and verify value is `ok`
       - DELETE the test key
       - Call `close_redis()` to clean up
       - Print PASS/FAIL
    5. **Print summary:**
       - Total checks passed/failed
       - If all pass: "All infrastructure connectivity checks PASSED"
       - If any fail: "FAILED: {list of failures}" and exit with code 1

    The script must:
    - Use `asyncio.run(main())` as entry point
    - Handle connection errors gracefully (try/except around each service check)
    - Print timestamps for each check
    - Work from the project root: `python scripts/verify_connectivity.py`

    **Important:** This script requires Docker services to be running AND Alembic migration to have been applied (from Plan 02). If the migration has not run yet (tables don't exist), the hypertable check will show 0 but the basic connection check still passes. The script should handle this gracefully (warn, don't fail on missing hypertables if basic connectivity works).

    Actually, since Plan 02 and Plan 03 are both Wave 2, the executor may run them in parallel or sequentially. The script should be resilient: if hypertables aren't created yet, it should WARN but not FAIL for the basic connectivity checks. Add a `--strict` flag that fails on missing hypertables (for use after Plan 02 is also done).

    Default mode: Check connectivity only (SELECT 1, PING). Print warnings for missing tables/hypertables.
    Strict mode (--strict): Check connectivity AND schema (10 tables, 7 hypertables). Fail if schema is incomplete.

    Add a `Makefile` target for this: `verify: python scripts/verify_connectivity.py --strict` (update Makefile from Plan 01 by appending this target).
  </action>
  <verify>
    Run: `python /home/user/Macro_Trading/scripts/verify_connectivity.py` (basic mode -- should pass if Docker is up)
    Run: `python /home/user/Macro_Trading/scripts/verify_connectivity.py --strict` (strict mode -- passes if both Plan 02 migration has run)
    Both commands print check results. Basic mode passes with all connectivity checks. Strict mode passes if migration has been applied.
  </verify>
  <done>
    verify_connectivity.py exists in scripts/ and can be run standalone. It tests async DB connection (asyncpg), sync DB connection (psycopg2), and Redis connection (set/get/delete). In default mode, it verifies basic connectivity. With --strict, it also validates 10 tables and 7 hypertables exist. All checks print PASS/FAIL with descriptive messages. Exit code 0 on success, 1 on any failure. Makefile has a `verify` target.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.core.database import async_engine, async_session_factory; print('DB OK')"` imports succeed
2. `python -c "from src.core.redis import get_redis, close_redis; print('Redis OK')"` imports succeed
3. `python scripts/verify_connectivity.py` passes all basic connectivity checks (async DB, sync DB, Redis)
4. `python scripts/verify_connectivity.py --strict` passes all checks including schema validation (requires Plan 02 migration)
</verification>

<success_criteria>
- database.py creates async engine (asyncpg) with pool_size=20 and sync engine (psycopg2) with pool_size=5
- Both session factories use autoflush=False and expire_on_commit=False
- get_async_session() is an async generator suitable for FastAPI Depends()
- redis.py provides singleton async Redis client with connection pool (max_connections=50)
- close_redis() properly cleans up client and pool for application shutdown
- Verification script proves live connectivity to TimescaleDB (async + sync) and Redis
</success_criteria>

<output>
After completion, create `.planning/phases/01-foundation/01-03-SUMMARY.md`
</output>
