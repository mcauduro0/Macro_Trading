---
phase: 19-dashboard-v2-api-expansion-testing-verification
plan: 04
type: execute
wave: 2
depends_on: ["19-03"]
files_modified:
  - tests/integration/test_pipeline_e2e.py
  - tests/integration/test_api_v1.py
  - tests/integration/test_api_v2.py
  - tests/integration/test_api_v3.py
  - .github/workflows/ci.yml
  - scripts/verify_phase2.py
autonomous: true
requirements:
  - TSTV-01
  - TSTV-02
  - TSTV-03
  - TSTV-04

must_haves:
  truths:
    - "Full pipeline E2E test runs the chain DB -> transforms -> agents -> strategies -> signals -> portfolio -> risk -> report without error"
    - "API v1 test validates health, macro/dashboard, agents, signals, portfolio/current, portfolio/risk endpoints return 200"
    - "API v2 test validates risk/var, risk/stress, risk/limits, portfolio/target, portfolio/rebalance-trades, portfolio/attribution, reports/daily endpoints return 200"
    - "API v3 test validates backtest/run, backtest/results, backtest/portfolio, backtest/comparison, strategies/{id}, strategies/{id}/signal/latest, strategies/{id}/signal/history, and WebSocket channels"
    - "CI/CD pipeline runs lint (ruff, black) and tests (unit + integration) on push and pull_request"
    - "Verification script checks all v3.0 components and outputs a formatted PASS/FAIL table"
  artifacts:
    - path: "tests/integration/test_pipeline_e2e.py"
      provides: "Full pipeline E2E integration test"
      contains: "test_full_pipeline"
    - path: "tests/integration/test_api_v1.py"
      provides: "API v1 endpoint integration tests"
      contains: "test_health"
    - path: "tests/integration/test_api_v2.py"
      provides: "API v2 endpoint integration tests"
      contains: "test_risk_var"
    - path: "tests/integration/test_api_v3.py"
      provides: "API v3 endpoint and WebSocket tests"
      contains: "test_backtest_run"
    - path: ".github/workflows/ci.yml"
      provides: "GitHub Actions CI/CD pipeline with lint, unit, and integration jobs"
      contains: "timescale/timescaledb"
    - path: "scripts/verify_phase2.py"
      provides: "Comprehensive v3.0 verification script"
      contains: "verify_"
  key_links:
    - from: "tests/integration/test_api_v3.py"
      to: "src/api/routes/backtest_api.py"
      via: "httpx AsyncClient requests"
      pattern: "backtest/run"
    - from: "tests/integration/test_api_v3.py"
      to: "src/api/routes/websocket_api.py"
      via: "websocket connection test"
      pattern: "ws/signals"
    - from: ".github/workflows/ci.yml"
      to: "tests/"
      via: "pytest command"
      pattern: "pytest"
    - from: "scripts/verify_phase2.py"
      to: "src/"
      via: "component import and validation"
      pattern: "StrategyRegistry|BacktestEngine"
---

<objective>
Create comprehensive integration tests (full pipeline E2E + all API endpoint tests grouped by version), GitHub Actions CI/CD pipeline (lint + test with service containers), and a verification script that validates all v3.0 components with formatted PASS/FAIL output.

Purpose: Ensures the complete v3.0 system works end-to-end, gates code quality via CI, and provides a one-command verification of all components.
Output: 4 test files, CI workflow config, and verification script.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/19-dashboard-v2-api-expansion-testing-verification/19-CONTEXT.md
@.planning/phases/19-dashboard-v2-api-expansion-testing-verification/19-03-SUMMARY.md
@src/api/main.py
@tests/integration/test_api_integration.py
@tests/integration/test_pipeline_integration.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Integration tests — pipeline E2E and API endpoint tests (v1, v2, v3)</name>
  <files>
    tests/integration/test_pipeline_e2e.py
    tests/integration/test_api_v1.py
    tests/integration/test_api_v2.py
    tests/integration/test_api_v3.py
  </files>
  <action>
**test_pipeline_e2e.py** — Full pipeline integration test:
- Uses pytest with asyncio marker
- Mocks external APIs: FRED, BCB, Yahoo -> provide fixture JSON/dict data; LLM API -> template fallback (set ANTHROPIC_API_KEY="" in env)
- Test function `test_full_pipeline_e2e`:
  1. **Transforms step**: Import and call curve/return/macro transforms with sample DataFrames. Assert output is non-empty.
  2. **Agents step**: Import AgentRegistry, call each agent's backtest_run() with a mock PointInTimeDataLoader returning sample data. Assert each returns AgentReport with signals list.
  3. **Strategies step**: Import StrategyRegistry, instantiate each strategy, call generate_signals() with sample date. Assert returns non-empty signals.
  4. **Signal aggregation step**: Import SignalAggregatorV2, aggregate mock signals. Assert produces AggregatedSignalV2 list.
  5. **Portfolio step**: Import PortfolioConstructor, build portfolio from aggregated signals. Assert produces PortfolioTarget.
  6. **Risk step**: Import VaRCalculator, compute VaR with sample returns. Assert produces VaRResult.
  7. **Report step**: Import DailyReportGenerator, generate with sample context. Assert produces non-empty markdown string.
- Each step wrapped in try/except: log step name + PASS/FAIL, collect results
- Final assertion: all steps passed
- Use `@pytest.mark.integration` marker

**test_api_v1.py** — API v1 endpoint tests:
- Use `httpx.AsyncClient` with `ASGITransport(app=app)` (FastAPI test pattern)
- Create a test app fixture that bypasses the DB lifespan (noop lifespan pattern from Phase 13)
- Tests (each is a separate async test function):
  - `test_health`: GET /health -> 200, response has "status"
  - `test_macro_dashboard`: GET /api/v1/macro/dashboard -> 200
  - `test_agents_signals`: GET /api/v1/agents/signals -> 200
  - `test_signals_dashboard`: GET /api/v1/signals/dashboard -> 200
  - `test_portfolio_current`: GET /api/v1/portfolio/current -> 200
  - `test_portfolio_risk`: GET /api/v1/portfolio/risk -> 200
  - `test_dashboard_html`: GET /dashboard -> 200, response contains "<!DOCTYPE html>" or "<html"
- All tests assert: status_code == 200, response JSON has "status" key (except HTML)

**test_api_v2.py** — API v2 endpoint tests:
- Same test client pattern as v1
- Tests:
  - `test_risk_var`: GET /api/v1/risk/var -> 200
  - `test_risk_stress`: GET /api/v1/risk/stress -> 200
  - `test_risk_limits`: GET /api/v1/risk/limits -> 200
  - `test_portfolio_target`: GET /api/v1/portfolio/target -> 200
  - `test_portfolio_rebalance_trades`: GET /api/v1/portfolio/rebalance-trades -> 200
  - `test_portfolio_attribution`: GET /api/v1/portfolio/attribution -> 200
  - `test_reports_daily`: GET /api/v1/reports/daily/latest -> 200
- Each asserts 200 status and valid response envelope

**test_api_v3.py** — API v3 endpoint and WebSocket tests:
- Same test client pattern
- REST tests:
  - `test_backtest_run`: POST /api/v1/backtest/run with {"strategy_id": "RATES_BR_01"} -> 200 or 202
  - `test_backtest_results`: GET /api/v1/backtest/results?strategy_id=RATES_BR_01 -> 200
  - `test_backtest_portfolio`: POST /api/v1/backtest/portfolio with {"strategy_ids": ["RATES_BR_01"]} -> 200 or 202
  - `test_backtest_comparison`: GET /api/v1/backtest/comparison?strategy_ids=RATES_BR_01,FX_BR_01 -> 200
  - `test_strategy_detail`: GET /api/v1/strategies/RATES_BR_01 -> 200, response has strategy_id
  - `test_strategy_signal_latest`: GET /api/v1/strategies/RATES_BR_01/signal/latest -> 200
  - `test_strategy_signal_history`: GET /api/v1/strategies/RATES_BR_01/signal/history -> 200
- WebSocket tests (use httpx WebSocket support or starlette.testclient.TestClient):
  - `test_ws_signals`: Connect to /ws/signals, assert connection accepted
  - `test_ws_portfolio`: Connect to /ws/portfolio, assert connection accepted
  - `test_ws_alerts`: Connect to /ws/alerts, assert connection accepted
- All marked with `@pytest.mark.integration`
  </action>
  <verify>
Run: `cd /home/user/Macro_Trading && python -m pytest tests/integration/test_api_v1.py tests/integration/test_api_v2.py tests/integration/test_api_v3.py -v --co` to list discovered test functions (collect-only, no execution).
Verify test count: expect 7 (v1) + 7 (v2) + 10 (v3) = 24+ tests discovered.
Run: `python -m pytest tests/integration/test_api_v1.py -v -x` to verify at least v1 tests pass.
  </verify>
  <done>
4 integration test files created: test_pipeline_e2e.py (7-step full pipeline chain), test_api_v1.py (7 health/data endpoints), test_api_v2.py (7 risk/portfolio/reports endpoints), test_api_v3.py (7 REST + 3 WebSocket tests). All use httpx.AsyncClient with noop lifespan for DB-independent testing.
  </done>
</task>

<task type="auto">
  <name>Task 2: CI/CD pipeline and verification script</name>
  <files>
    .github/workflows/ci.yml
    scripts/verify_phase2.py
  </files>
  <action>
**ci.yml** — Create GitHub Actions CI/CD workflow:
```yaml
name: CI

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]

jobs:
  lint:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install ruff black
      - run: ruff check src/ tests/
      - run: black --check src/ tests/

  test:
    runs-on: ubuntu-latest
    needs: lint
    services:
      timescaledb:
        image: timescale/timescaledb:latest-pg15
        env:
          POSTGRES_USER: macro
          POSTGRES_PASSWORD: macro
          POSTGRES_DB: macro_trading
        ports:
          - 5432:5432
        options: >-
          --health-cmd pg_isready
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
      redis:
        image: redis:7
        ports:
          - 6379:6379
        options: >-
          --health-cmd "redis-cli ping"
          --health-interval 10s
          --health-timeout 5s
          --health-retries 5
    env:
      DATABASE_URL: postgresql+asyncpg://macro:macro@localhost:5432/macro_trading
      SYNC_DATABASE_URL: postgresql+psycopg2://macro:macro@localhost:5432/macro_trading
      REDIS_URL: redis://localhost:6379/0
    steps:
      - uses: actions/checkout@v4
      - uses: actions/setup-python@v5
        with:
          python-version: '3.11'
      - run: pip install -e ".[dev]" || pip install -r requirements.txt && pip install pytest pytest-asyncio pytest-cov httpx
      - name: Run unit tests
        run: python -m pytest tests/ -v --ignore=tests/integration -x
      - name: Run integration tests
        run: python -m pytest tests/integration/ -v -x
```

Create `.github/` and `.github/workflows/` directories if they don't exist.

**verify_phase2.py** — Comprehensive v3.0 verification script:
- Standalone script (no pytest dependency, uses stdlib only + project imports)
- Add `sys.path.insert(0, project_root)` for standalone execution
- Define a `CheckResult` namedtuple: (name, status, detail)
- Run 12+ checks, each in a try/except returning CheckResult:

1. **StrategyRegistry**: Import StrategyRegistry, list_all() -> assert >= 24 strategies
2. **Agents**: Import AgentRegistry, check 5 agents registered (inflation, monetary, fiscal, fx, cross_asset)
3. **Signal Aggregation**: Import SignalAggregatorV2, verify 3 methods exist (confidence_weighted, rank_based, bayesian)
4. **Monte Carlo VaR**: Import VaRCalculator, call with sample returns (numpy random seed=42), verify produces VaRResult
5. **Stress Scenarios**: Import StressTester, verify DEFAULT_SCENARIOS has 6+ entries
6. **Black-Litterman**: Import BlackLitterman, verify class exists with optimize method
7. **Dagster Assets**: Check src/orchestration/dagster_assets/ directory has asset files, count definitions (grep for @asset decorator across files) -> assert >= 20
8. **Grafana Dashboards**: Check monitoring/grafana/dashboards/ has 4 JSON files
9. **Alert Rules**: Import AlertManager, verify 10 default rules configured
10. **Dashboard HTML**: Check src/api/static/dashboard.html exists and contains "text/babel"
11. **API Endpoints**: Import app from src.api.main, enumerate routes, verify key endpoints exist: /health, /api/v1/backtest/run, /api/v1/strategies, /ws/signals, /ws/portfolio, /ws/alerts
12. **WebSocket Channels**: Import ConnectionManager from websocket_api, verify 3 channels configurable

- Output: formatted table using box-drawing characters:
  ```
  ╔══════════════════════════════════════╦════════╦══════════════════╗
  ║ Component                            ║ Status ║ Detail           ║
  ╠══════════════════════════════════════╬════════╬══════════════════╣
  ║ StrategyRegistry (24+ strategies)    ║ PASS   ║ 24 registered    ║
  ║ ...                                  ║ ...    ║ ...              ║
  ╚══════════════════════════════════════╩════════╩══════════════════╝
  ```
- Color-code PASS (green ANSI) and FAIL (red ANSI) if terminal supports it
- Print summary: "X/Y checks passed"
- Exit code: 0 if all pass, 1 if any fail
- Make script executable and add shebang `#!/usr/bin/env python3`
  </action>
  <verify>
Run: `cd /home/user/Macro_Trading && python scripts/verify_phase2.py` — expect formatted output with PASS/FAIL per component.
Verify .github/workflows/ci.yml exists and is valid YAML: `python -c "import yaml; yaml.safe_load(open('.github/workflows/ci.yml'))"`
Check verify_phase2.py exit code: `python scripts/verify_phase2.py; echo "Exit code: $?"`
  </verify>
  <done>
CI/CD pipeline (.github/workflows/ci.yml) configured with lint (ruff + black) and test (unit + integration with TimescaleDB and Redis service containers) jobs. Verification script (scripts/verify_phase2.py) checks 12 v3.0 components with formatted PASS/FAIL table and exit code 0/1.
  </done>
</task>

</tasks>

<verification>
1. test_pipeline_e2e.py tests full data flow chain (7 steps)
2. test_api_v1.py tests 7 v1 endpoints
3. test_api_v2.py tests 7 v2 endpoints
4. test_api_v3.py tests 7 REST + 3 WebSocket v3 endpoints
5. ci.yml has lint and test jobs with service containers
6. verify_phase2.py checks 12+ components with formatted output
7. All test files use consistent patterns (noop lifespan, httpx AsyncClient)
</verification>

<success_criteria>
- `pytest tests/integration/ --co` discovers 24+ test functions
- `python scripts/verify_phase2.py` outputs formatted table and exits with code 0 or 1
- `.github/workflows/ci.yml` is valid YAML with lint and test jobs
- API tests use no real database (noop lifespan + sample data)
</success_criteria>

<output>
After completion, create `.planning/phases/19-dashboard-v2-api-expansion-testing-verification/19-04-SUMMARY.md`
</output>
