---
phase: 16-crossasset-nlp
plan: 02
type: execute
wave: 1
depends_on: []
files_modified:
  - src/nlp/__init__.py
  - src/nlp/scrapers/__init__.py
  - src/nlp/scrapers/copom_scraper.py
  - src/nlp/scrapers/fomc_scraper.py
  - src/core/models/nlp_documents.py
  - src/core/models/__init__.py
  - alembic/versions/007_create_nlp_documents_table.py
  - tests/test_copom_scraper.py
  - tests/test_fomc_scraper.py
autonomous: true
requirements:
  - NLP-01
  - NLP-02
  - NLP-05

must_haves:
  truths:
    - "COPOMScraper retrieves atas and comunicados from bcb.gov.br with incremental caching (first run scrapes, subsequent runs check for new documents only)"
    - "FOMCScraper retrieves statements and minutes from federalreserve.gov with incremental caching"
    - "Both scrapers cover documents from 2010-present and store raw text with metadata"
    - "nlp_documents table exists via Alembic migration with all required columns (source, doc_type, date, raw_text, hawk_score, dove_score, change_score, key_phrases, created_at)"
  artifacts:
    - path: "src/nlp/scrapers/copom_scraper.py"
      provides: "COPOMScraper for BCB documents"
      contains: "COPOMScraper"
    - path: "src/nlp/scrapers/fomc_scraper.py"
      provides: "FOMCScraper for Fed documents"
      contains: "FOMCScraper"
    - path: "src/core/models/nlp_documents.py"
      provides: "NlpDocumentRecord ORM model"
      contains: "NlpDocumentRecord"
    - path: "alembic/versions/007_create_nlp_documents_table.py"
      provides: "Alembic migration for nlp_documents table"
      contains: "nlp_documents"
    - path: "tests/test_copom_scraper.py"
      provides: "Unit tests for COPOMScraper"
    - path: "tests/test_fomc_scraper.py"
      provides: "Unit tests for FOMCScraper"
  key_links:
    - from: "src/nlp/scrapers/copom_scraper.py"
      to: "src/core/models/nlp_documents.py"
      via: "persist_documents() stores scraped docs to nlp_documents"
      pattern: "NlpDocumentRecord"
    - from: "src/nlp/scrapers/fomc_scraper.py"
      to: "src/core/models/nlp_documents.py"
      via: "persist_documents() stores scraped docs to nlp_documents"
      pattern: "NlpDocumentRecord"
---

<objective>
Build the NLP document scraping infrastructure: COPOMScraper for BCB communications, FOMCScraper for Federal Reserve communications, and the nlp_documents database table for persistent storage.

Purpose: Central bank communications are the primary input to the hawk/dove sentiment analysis pipeline (Plan 16-03). This plan creates the data acquisition layer -- scrapers that retrieve and store raw document text from both institutions.

Output: Two scraper classes with incremental caching, NlpDocumentRecord ORM model, Alembic migration 007, comprehensive unit tests with mocked HTTP.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/16-crossasset-nlp/16-CONTEXT.md
@src/core/models/__init__.py
@src/core/models/base.py
@alembic/versions/006_add_strategy_state_enhance_backtest_results.py
@src/connectors/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: NLP document model, migration, and scraper package scaffold</name>
  <files>
    src/nlp/__init__.py
    src/nlp/scrapers/__init__.py
    src/core/models/nlp_documents.py
    src/core/models/__init__.py
    alembic/versions/007_create_nlp_documents_table.py
  </files>
  <action>
**1. Create `src/nlp/__init__.py`** -- package init, empty for now.

**2. Create `src/nlp/scrapers/__init__.py`** -- package init exporting COPOMScraper, FOMCScraper (lazy imports to avoid circular).

**3. Create `src/core/models/nlp_documents.py`** -- NlpDocumentRecord ORM model.

Per locked decision: single nlp_documents table with all columns inline.

```python
class NlpDocumentRecord(Base):
    __tablename__ = "nlp_documents"

    id = Column(Integer, primary_key=True, autoincrement=True)
    source = Column(String(20), nullable=False)        # "copom" or "fomc"
    doc_type = Column(String(30), nullable=False)       # "ata", "comunicado", "statement", "minutes"
    doc_date = Column(Date, nullable=False)             # meeting/release date
    raw_text = Column(Text, nullable=True)              # full document text
    hawk_score = Column(Float, nullable=True)           # [-1, +1], null until scored
    dove_score = Column(Float, nullable=True)           # [-1, +1], null until scored
    change_score = Column(String(30), nullable=True)    # categorical: hawkish_shift, dovish_shift, neutral, major_hawkish_shift, major_dovish_shift
    key_phrases = Column(JSONB, nullable=True)          # JSON list of extracted phrases
    url = Column(String(500), nullable=True)            # source URL
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False)
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=True)
```

Add UniqueConstraint on (source, doc_type, doc_date) named "uq_nlp_documents_natural_key" to prevent duplicate scrapes.

**4. Update `src/core/models/__init__.py`** -- add NlpDocumentRecord to exports.

**5. Create `alembic/versions/007_create_nlp_documents_table.py`:**
- Revision ID: "g7h8i9j0k1l2"
- Down revision: "f6g7h8i9j0k1" (migration 006)
- Create nlp_documents table with all columns
- Create unique constraint on (source, doc_type, doc_date)
- Create index on (source, doc_date) for efficient lookups
- downgrade: drop table
  </action>
  <verify>
    Run: `python -c "from src.core.models.nlp_documents import NlpDocumentRecord; print(NlpDocumentRecord.__tablename__)"`
    Run: `python -c "from src.nlp.scrapers import COPOMScraper, FOMCScraper; print('NLP package OK')"`
    Run: `python -c "from alembic.versions import __path__; print('Migration files exist')"`
  </verify>
  <done>
    NlpDocumentRecord ORM model with all required columns, Alembic migration 007, src/nlp package scaffold. All imports resolve cleanly.
  </done>
</task>

<task type="auto">
  <name>Task 2: COPOMScraper, FOMCScraper with incremental caching and tests</name>
  <files>
    src/nlp/scrapers/copom_scraper.py
    src/nlp/scrapers/fomc_scraper.py
    tests/test_copom_scraper.py
    tests/test_fomc_scraper.py
  </files>
  <action>
**1. Create `src/nlp/scrapers/copom_scraper.py`** (NLP-01):

Per locked decision: HTTP + incremental cache -- first run scrapes, subsequent runs check for new documents only.

COPOMScraper class:
- `__init__(self, cache_dir: str = ".cache/nlp/copom")` -- creates cache directory if not exists
- Uses httpx for HTTP requests (already in dependencies)
- User-Agent header: "MacroTradingSystem/1.0"
- Rate limiting: 2 seconds between requests

Key methods:
- `scrape_atas(self, start_year: int = 2010) -> list[ScrapedDocument]`:
  - BCB COPOM atas URL pattern: `https://www.bcb.gov.br/publicacoes/atascopom`
  - Parse the index page listing ata links
  - For each ata link not already cached: fetch full text, extract body content
  - Cache document locally as JSON (date, url, raw_text, doc_type="ata")
  - Return list of ScrapedDocument(source="copom", doc_type="ata", date, raw_text, url)

- `scrape_comunicados(self, start_year: int = 2010) -> list[ScrapedDocument]`:
  - BCB COPOM comunicados (post-meeting statements)
  - URL pattern: `https://www.bcb.gov.br/publicacoes/comunicadoscopom`
  - Same incremental pattern as atas

- `scrape_all(self, start_year: int = 2010) -> list[ScrapedDocument]`:
  - Calls scrape_atas + scrape_comunicados
  - Returns combined list sorted by date

- `get_cached_documents(self) -> list[ScrapedDocument]`:
  - Load all cached documents without making HTTP requests

- `persist_documents(self, documents: list[ScrapedDocument], session) -> int`:
  - Bulk insert into nlp_documents via ON CONFLICT DO NOTHING
  - Returns count of new documents inserted

ScrapedDocument dataclass: source, doc_type, doc_date, raw_text, url

HTML parsing approach (Claude's discretion):
- Use Python stdlib html.parser or regex for simple HTML extraction
- BCB pages have predictable structure -- extract text from content div
- Strip HTML tags, normalize whitespace
- If page structure changes, log warning and skip document

Incremental logic:
- On each scrape call, check cache_dir for existing JSON files
- Only fetch documents with dates not already in cache
- Cache files named: `{source}_{doc_type}_{YYYY-MM-DD}.json`

**2. Create `src/nlp/scrapers/fomc_scraper.py`** (NLP-02):

FOMCScraper class:
- `__init__(self, cache_dir: str = ".cache/nlp/fomc")`
- Same HTTP pattern as COPOMScraper

Key methods:
- `scrape_statements(self, start_year: int = 2010) -> list[ScrapedDocument]`:
  - Fed statements URL: `https://www.federalreserve.gov/monetarypolicy/fomccalendars.htm`
  - Historical: `https://www.federalreserve.gov/monetarypolicy/fomchistorical{year}.htm`
  - Parse index pages for statement links
  - Fetch and extract text from each statement page
  - doc_type="statement"

- `scrape_minutes(self, start_year: int = 2010) -> list[ScrapedDocument]`:
  - Fed minutes from same calendar pages
  - doc_type="minutes"

- `scrape_all(self, start_year: int = 2010) -> list[ScrapedDocument]`:
  - Combined, sorted by date

- `get_cached_documents(self) -> list[ScrapedDocument]`
- `persist_documents(self, documents, session) -> int`

Same incremental caching pattern. Fed pages also have predictable HTML structure.

**3. Create `tests/test_copom_scraper.py`:**
- Mock httpx responses with sample HTML snippets
- Test scrape_atas returns ScrapedDocument list with correct fields
- Test incremental cache: second call with same cache returns no new HTTP requests
- Test get_cached_documents loads from cache
- Test persist_documents creates NlpDocumentRecord objects
- Use tmp_path fixture for cache directory isolation
- 8-10 tests

**4. Create `tests/test_fomc_scraper.py`:**
- Same pattern as COPOM tests with Fed HTML snippets
- Test scrape_statements, scrape_minutes
- Test incremental caching
- 8-10 tests

IMPORTANT: Tests must NOT make real HTTP requests. Use unittest.mock.patch or respx to mock all HTTP calls. Use `tmp_path` for cache directory isolation.
  </action>
  <verify>
    Run: `python -m pytest tests/test_copom_scraper.py tests/test_fomc_scraper.py -v`
    Expected: All tests pass without making real HTTP requests.
    Run: `python -c "from src.nlp.scrapers.copom_scraper import COPOMScraper; s = COPOMScraper(cache_dir='/tmp/test_copom'); print(type(s))"`
    Run: `python -c "from src.nlp.scrapers.fomc_scraper import FOMCScraper; s = FOMCScraper(cache_dir='/tmp/test_fomc'); print(type(s))"`
  </verify>
  <done>
    COPOMScraper retrieves COPOM atas and comunicados from bcb.gov.br with incremental caching. FOMCScraper retrieves FOMC statements and minutes from federalreserve.gov with incremental caching. Both cover 2010-present. All tests pass with mocked HTTP. persist_documents wires to NlpDocumentRecord.
  </done>
</task>

</tasks>

<verification>
1. `python -c "from src.core.models.nlp_documents import NlpDocumentRecord; print(NlpDocumentRecord.__tablename__)"` -- prints "nlp_documents"
2. `python -c "from src.nlp.scrapers.copom_scraper import COPOMScraper"` -- imports OK
3. `python -c "from src.nlp.scrapers.fomc_scraper import FOMCScraper"` -- imports OK
4. `python -m pytest tests/test_copom_scraper.py tests/test_fomc_scraper.py -v` -- all pass
5. `python -m pytest tests/ -x --timeout=120` -- existing tests still pass (no regressions)
</verification>

<success_criteria>
- COPOMScraper scrapes atas and comunicados with incremental cache (NLP-01)
- FOMCScraper scrapes statements and minutes with incremental cache (NLP-02)
- nlp_documents table defined in ORM and Alembic migration (NLP-05)
- All scraper tests pass with mocked HTTP
- No regressions in existing tests
</success_criteria>

<output>
After completion, create `.planning/phases/16-crossasset-nlp/16-02-SUMMARY.md`
</output>
