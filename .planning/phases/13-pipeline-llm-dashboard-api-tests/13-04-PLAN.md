---
phase: 13-pipeline-llm-dashboard-api-tests
plan: 04
type: execute
wave: 2
depends_on: [13-01, 13-02]
files_modified:
  - src/api/routes/agents.py
  - src/api/routes/signals.py
  - src/api/routes/strategies_api.py
  - src/api/routes/portfolio_api.py
  - src/api/routes/risk_api.py
  - src/api/routes/reports.py
  - src/api/main.py
  - tests/test_api/test_v2_endpoints.py
  - tests/test_integration/__init__.py
  - tests/test_integration/test_pipeline_integration.py
  - tests/test_integration/test_api_integration.py
  - scripts/verify_infrastructure.py
  - Makefile
autonomous: true
requirements: [APIV2-01, APIV2-02, APIV2-03, APIV2-04, APIV2-05, APIV2-06, APIV2-07, APIV2-08, APIV2-09, TESTV2-05, TESTV2-06, TESTV2-07]

must_haves:
  truths:
    - "GET /api/v1/agents returns 200 with list of registered agents"
    - "GET /api/v1/agents/{agent_id}/latest returns 200 with latest AgentReport"
    - "POST /api/v1/agents/{agent_id}/run returns 200 and triggers agent execution"
    - "GET /api/v1/signals/latest returns 200 with signals and consensus"
    - "GET /api/v1/strategies returns 200 with all 8 strategies"
    - "GET /api/v1/strategies/{strategy_id}/backtest returns 200 with results"
    - "GET /api/v1/portfolio/current returns 200 with positions"
    - "GET /api/v1/portfolio/risk returns 200 with risk report"
    - "GET /api/v1/reports/daily-brief returns 200 with macro narrative"
    - "Integration test runs full pipeline for a known date without error"
    - "All API endpoints (v1 + v2) return 200 OK via TestClient"
    - "Verification script validates Phase 0 + Phase 1 components"
  artifacts:
    - path: "src/api/routes/agents.py"
      provides: "3 agent endpoints (list, latest, run)"
      contains: "router"
    - path: "src/api/routes/signals.py"
      provides: "1 signals endpoint (latest)"
      contains: "router"
    - path: "src/api/routes/strategies_api.py"
      provides: "2 strategy endpoints (list, backtest)"
      contains: "router"
    - path: "src/api/routes/portfolio_api.py"
      provides: "2 portfolio endpoints (current, risk)"
      contains: "router"
    - path: "src/api/routes/reports.py"
      provides: "1 reports endpoint (daily-brief)"
      contains: "router"
    - path: "tests/test_integration/test_pipeline_integration.py"
      provides: "Full pipeline integration test"
      min_lines: 50
    - path: "tests/test_integration/test_api_integration.py"
      provides: "All-endpoints integration test"
      min_lines: 60
  key_links:
    - from: "src/api/main.py"
      to: "src/api/routes/agents.py"
      via: "app.include_router(agents.router, prefix='/api/v1')"
      pattern: "agents.router"
    - from: "src/api/routes/reports.py"
      to: "src/narrative/generator.py"
      via: "NarrativeGenerator.generate() for daily brief"
      pattern: "NarrativeGenerator"
    - from: "tests/test_integration/test_pipeline_integration.py"
      to: "src/pipeline/daily_pipeline.py"
      via: "DailyPipeline.run() end-to-end"
      pattern: "DailyPipeline"
---

<objective>
Build 9 new API endpoints (agents, signals, strategies, portfolio, risk, daily-brief), integration tests validating the full pipeline and all API endpoints, update the verification script for Phase 0 + Phase 1 coverage, and add Makefile targets.

Purpose: The API endpoints make all v2.0 data accessible via REST for the dashboard and external consumers. Integration tests validate that the entire system works end-to-end. The verification script provides a single command to validate the complete system.

Output: 6 new route modules, updated main.py, integration tests, updated verification script, Makefile targets.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-pipeline-llm-dashboard-api-tests/13-CONTEXT.md
@.planning/phases/13-pipeline-llm-dashboard-api-tests/13-01-SUMMARY.md
@.planning/phases/13-pipeline-llm-dashboard-api-tests/13-02-SUMMARY.md
@src/api/main.py
@src/api/routes/health.py
@src/api/routes/macro.py
@src/agents/registry.py
@src/agents/base.py
@src/strategies/__init__.py
@src/portfolio/__init__.py
@src/risk/__init__.py
@src/narrative/__init__.py
@src/pipeline/__init__.py
@scripts/verify_infrastructure.py
@Makefile
</context>

<tasks>

<task type="auto">
  <name>Task 1: 9 API endpoints with consistent response envelope and main.py registration</name>
  <files>
    src/api/routes/agents.py
    src/api/routes/signals.py
    src/api/routes/strategies_api.py
    src/api/routes/portfolio_api.py
    src/api/routes/risk_api.py
    src/api/routes/reports.py
    src/api/main.py
  </files>
  <action>
All endpoints use the consistent response envelope from CONTEXT.md:
```json
{"status": "ok", "data": {...}, "meta": {"timestamp": "2024-01-15T08:00:00Z"}}
```
All endpoints accept optional `?date=YYYY-MM-DD` query parameter (defaults to latest/today).

Create a shared helper in each module (or a small utility):
```python
def _envelope(data: Any) -> dict:
    return {"status": "ok", "data": data, "meta": {"timestamp": datetime.now(timezone.utc).isoformat()}}
```

**1. Agents routes (`src/api/routes/agents.py`):**

`GET /api/v1/agents` (APIV2-01):
- Register all 5 agents, return list of: agent_id, agent_name, description, execution_order_index.
- For each agent, include `last_run` (None if not yet run) and `signal_count` (0 if not yet run).
- Use AgentRegistry to get registered agents.

`GET /api/v1/agents/{agent_id}/latest` (APIV2-02):
- Accept `agent_id` path param and optional `?date=YYYY-MM-DD`.
- Run agent for the given date (or latest available) and return AgentReport as JSON.
- Report fields: agent_id, as_of_date, narrative, signals (list of: signal_id, direction, strength, confidence, value, horizon_days, metadata).
- Return 404 if agent_id not found.

`POST /api/v1/agents/{agent_id}/run` (APIV2-03):
- Accept `agent_id` path param and optional `date` in request body.
- Run the specific agent via AgentRegistry.get(agent_id).run(date).
- Return the AgentReport in envelope format.
- Return 404 if agent_id not found, 500 on execution error.

**2. Signals route (`src/api/routes/signals.py`):**

`GET /api/v1/signals/latest` (APIV2-04):
- Run all agents for the given date (or today), collect all signals.
- Compute consensus: for each asset class direction, count LONG/SHORT/NEUTRAL signals weighted by confidence.
- Return: signals (flat list of all agent signals), consensus (per asset class: direction, avg_confidence, agreement_ratio).

**3. Strategies routes (`src/api/routes/strategies_api.py`):**

`GET /api/v1/strategies` (APIV2-05):
- Return list of all 8 strategies from ALL_STRATEGIES: strategy_id, class name, description (from docstring), asset_class, instruments, status ("active").

`GET /api/v1/strategies/{strategy_id}/backtest` (APIV2-06):
- Accept strategy_id path param.
- Query backtest_results table for latest result (if available) or return placeholder.
- Return: strategy_id, sharpe_ratio, annual_return, max_drawdown, win_rate, profit_factor, equity_curve (list of {date, equity}).
- Return 404 if strategy_id not found in ALL_STRATEGIES.

**4. Portfolio routes (`src/api/routes/portfolio_api.py`):**

`GET /api/v1/portfolio/current` (APIV2-07):
- Return consolidated portfolio positions.
- For each position: instrument, direction (LONG/SHORT), weight (%), contributing_strategy_ids, asset_class.
- Include summary: total_positions, net_leverage, gross_leverage.

`GET /api/v1/portfolio/risk` (APIV2-08):
- Return risk report: VaR (95%, 99%), CVaR, stress test results (scenario name + P&L), limit utilization, circuit_breaker_status, risk_level.
- Uses RiskMonitor report format.

**5. Reports route (`src/api/routes/reports.py`):**

`GET /api/v1/reports/daily-brief` (APIV2-09):
- Generate daily macro brief via NarrativeGenerator.
- Run agents for the date, pass reports to NarrativeGenerator.generate().
- Return: content (narrative text), source ("llm" or "template"), word_count, generated_at, as_of_date.

**Update main.py:**
- Add imports for all 6 new route modules: agents, signals, strategies_api, portfolio_api, risk_api, reports
- Register all routers with `prefix="/api/v1"`:
  ```python
  app.include_router(agents.router, prefix="/api/v1")
  app.include_router(signals.router, prefix="/api/v1")
  app.include_router(strategies_api.router, prefix="/api/v1")
  app.include_router(portfolio_api.router, prefix="/api/v1")
  app.include_router(risk_api.router, prefix="/api/v1")
  app.include_router(reports.router, prefix="/api/v1")
  ```

**Important:** All endpoint handlers should catch exceptions from underlying modules gracefully and return appropriate HTTP error codes (404 for not found, 500 for internal errors) within the envelope pattern: `{"status": "error", "error": "message", "meta": {"timestamp": ...}}`.

**Note on data access:** Since the agents, strategies, portfolio, and risk modules are pure computation modules (no DB), endpoints will instantiate and run them on-the-fly for the requested date. For endpoints that need historical data (backtest results), query the DB. Use sync execution wrapped in `asyncio.to_thread()` where needed since agents use sync DB sessions.
  </action>
  <verify>
python -c "from src.api.routes import agents, signals, strategies_api, portfolio_api, risk_api, reports; print('All route modules import OK')"
python -c "from src.api.main import app; routes = [r.path for r in app.routes]; print(f'{len(routes)} routes registered')"
  </verify>
  <done>
9 API endpoints registered: 3 agent, 1 signals, 2 strategies, 2 portfolio, 1 reports. All use consistent response envelope. main.py includes all 6 new route modules.
  </done>
</task>

<task type="auto">
  <name>Task 2: Integration tests (pipeline + API), verification script update, Makefile targets</name>
  <files>
    tests/test_integration/__init__.py
    tests/test_integration/test_pipeline_integration.py
    tests/test_integration/test_api_integration.py
    tests/test_api/test_v2_endpoints.py
    scripts/verify_infrastructure.py
    Makefile
  </files>
  <action>
**Pipeline integration test (`tests/test_integration/test_pipeline_integration.py`) — TESTV2-05:**

End-to-end pipeline test for a known date:

1. `test_full_pipeline_dry_run` — Run `DailyPipeline(as_of_date=date(2024, 1, 15), dry_run=True).run()`. Mock the data layer (PointInTimeDataLoader, DB connections) to return realistic synthetic data. Verify:
   - Pipeline completes without error
   - PipelineResult.status == "success"
   - All 8 step_timings keys present
   - signal_count > 0
   - position_count >= 0
   - regime is a valid string

2. `test_pipeline_agent_to_risk_chain` — Verify the data flows correctly: agents produce signals -> aggregation produces consensus -> strategies produce positions -> portfolio constructs targets -> risk generates report. Mock data layer, verify each intermediate result is non-empty.

3. `test_pipeline_abort_on_agent_failure` — Mock one agent to raise, verify pipeline aborts with FAILED status.

Use `unittest.mock.patch` extensively to mock:
- `PointInTimeDataLoader` (return synthetic DataFrames)
- DB sessions (sync and async)
- Any external I/O

**API integration test (`tests/test_integration/test_api_integration.py`) — TESTV2-06:**

Using FastAPI TestClient, verify all endpoints return 200:

1. `test_all_v1_endpoints_return_200` — Iterate over all original v1 endpoints: /health, /api/v1/macro/dashboard, /api/v1/macro/series, /api/v1/curves/latest, /api/v1/market-data/latest, /api/v1/flows/latest. Each must return 200 (mock DB as needed).

2. `test_all_v2_endpoints_return_200` — Iterate over all 9 new endpoints:
   - GET /api/v1/agents
   - GET /api/v1/agents/inflation_agent/latest
   - POST /api/v1/agents/inflation_agent/run
   - GET /api/v1/signals/latest
   - GET /api/v1/strategies
   - GET /api/v1/strategies/RATES_BR_01/backtest
   - GET /api/v1/portfolio/current
   - GET /api/v1/portfolio/risk
   - GET /api/v1/reports/daily-brief
   Mock underlying agent/strategy execution to return synthetic results.

3. `test_dashboard_endpoint` — GET /dashboard returns 200.

4. `test_response_envelope_format` — Verify all v2 GET endpoints return `{"status": "ok", "data": ..., "meta": {"timestamp": ...}}` format.

5. `test_404_for_unknown_agent` — GET /api/v1/agents/nonexistent/latest returns 404.

**V2 endpoint unit tests (`tests/test_api/test_v2_endpoints.py`):**

Focused unit tests for each new endpoint with mocked dependencies:

1. `test_agents_list_returns_5_agents` — GET /api/v1/agents returns data with 5 agent entries.
2. `test_agent_latest_returns_report` — Mock agent run, verify response contains signals list.
3. `test_agent_run_post` — POST triggers execution and returns report.
4. `test_signals_latest_has_consensus` — Verify consensus field in response.
5. `test_strategies_list_returns_8` — GET /api/v1/strategies returns 8 strategies.
6. `test_strategy_backtest_returns_metrics` — Verify sharpe_ratio, max_drawdown fields.
7. `test_portfolio_current_has_positions` — Verify positions list in response.
8. `test_portfolio_risk_has_var` — Verify var_95 field in response.
9. `test_daily_brief_has_content` — Verify content and source fields.
10. `test_date_parameter_parsing` — `?date=2024-01-15` is correctly parsed and passed to underlying logic.

**Verification script update (`scripts/verify_infrastructure.py`) — TESTV2-07:**

Add a new section to the existing verification script for Phase 1 (v2.0) components. After the existing Phase 0 checks, add:

```
═══ Phase 1 (v2.0): Quantitative Models & Agents ═══
```

Checks to add:
- Agent framework imports (BaseAgent, AgentRegistry, AgentReport, AgentSignal)
- All 5 agents importable (InflationAgent, MonetaryPolicyAgent, FiscalAgent, FxEquilibriumAgent, CrossAssetAgent)
- Strategy framework imports (BaseStrategy, ALL_STRATEGIES with 8 entries)
- Backtesting engine imports (BacktestEngine, BacktestResult, Portfolio)
- Portfolio construction imports (SignalAggregator, PortfolioConstructor, CapitalAllocator)
- Risk management imports (VaRCalculator, StressTester, RiskMonitor, DrawdownManager)
- Pipeline imports (DailyPipeline, PipelineResult)
- Narrative imports (NarrativeGenerator, NarrativeBrief)
- API routes count (should be > 15 total routes)

Each check wrapped in try/except (existing pattern). Print pass/fail for each.

**Makefile additions:**

Add to the existing Makefile after the Daily Pipeline section:

```makefile
# ── Integration Tests ────────────────────────────────────────────────
# Run integration tests only
test-integration:
	pytest tests/test_integration/ -v

# Run all tests (unit + integration)
test-all:
	pytest tests/ -v --cov=src
```

Update `.PHONY` line to include `test-integration test-all`.
  </action>
  <verify>
pytest tests/test_integration/ -v  # integration tests pass
pytest tests/test_api/test_v2_endpoints.py -v  # endpoint tests pass
python scripts/verify_infrastructure.py --quick 2>&1 | grep -c "OK"  # multiple checks pass
make -n test-integration  # shows correct command
  </verify>
  <done>
Integration tests validate full pipeline and all API endpoints return 200, v2 endpoint unit tests pass, verification script covers Phase 0 + Phase 1 components, Makefile has test-integration and test-all targets.
  </done>
</task>

</tasks>

<verification>
1. All 9 new API endpoints return 200 via TestClient
2. Integration test runs full pipeline (agents -> strategies -> portfolio -> risk) without error
3. All API endpoints (v1 + v2) return 200 in integration test
4. Response envelope format verified: {"status": "ok", "data": ..., "meta": {...}}
5. Verification script includes Phase 1 component checks
6. `pytest tests/test_integration/ tests/test_api/test_v2_endpoints.py -v` — all pass
7. `make -n test-integration` shows correct command
</verification>

<success_criteria>
- 9 new API endpoints registered and returning 200 OK
- Consistent response envelope on all v2 endpoints
- Integration test: full pipeline runs for known date without error (TESTV2-05)
- Integration test: all API endpoints (v1 + v2) return 200 OK (TESTV2-06)
- Verification script validates Phase 0 + Phase 1 components end-to-end (TESTV2-07)
- All endpoint unit tests pass
- Makefile targets: test-integration, test-all
</success_criteria>

<output>
After completion, create `.planning/phases/13-pipeline-llm-dashboard-api-tests/13-04-SUMMARY.md`
</output>
