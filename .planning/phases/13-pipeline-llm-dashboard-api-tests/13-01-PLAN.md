---
phase: 13-pipeline-llm-dashboard-api-tests
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/daily_run.py
  - src/pipeline/__init__.py
  - src/pipeline/daily_pipeline.py
  - alembic/versions/xxx_create_pipeline_runs.py
  - tests/test_pipeline/__init__.py
  - tests/test_pipeline/test_daily_pipeline.py
  - Makefile
autonomous: true
requirements: [PIPE-01, PIPE-02, PIPE-03]

must_haves:
  truths:
    - "python scripts/daily_run.py --date 2024-01-15 --dry-run completes 8 steps in sequence without error"
    - "Each pipeline step prints name, duration, and status in CI build log style"
    - "Pipeline aborts immediately on any step failure with clear error message"
    - "End-of-run summary shows signal count, top positions, portfolio leverage, VaR, and active risk alerts"
    - "Pipeline run metadata is persisted to pipeline_runs table (skipped in dry-run mode)"
  artifacts:
    - path: "scripts/daily_run.py"
      provides: "CLI entry point with --date and --dry-run arguments"
      contains: "argparse"
    - path: "src/pipeline/daily_pipeline.py"
      provides: "DailyPipeline class with 8-step orchestration"
      contains: "class DailyPipeline"
    - path: "src/pipeline/__init__.py"
      provides: "Package exports"
    - path: "tests/test_pipeline/test_daily_pipeline.py"
      provides: "Unit tests for pipeline steps and summary output"
      min_lines: 80
  key_links:
    - from: "scripts/daily_run.py"
      to: "src/pipeline/daily_pipeline.py"
      via: "DailyPipeline import and run()"
      pattern: "DailyPipeline"
    - from: "src/pipeline/daily_pipeline.py"
      to: "src/agents/registry.py"
      via: "AgentRegistry.run_all() in agents step"
      pattern: "AgentRegistry"
    - from: "src/pipeline/daily_pipeline.py"
      to: "src/risk/risk_monitor.py"
      via: "RiskMonitor.generate_report() in risk step"
      pattern: "RiskMonitor"
---

<objective>
Build the daily orchestration pipeline that executes 8 steps in sequence (ingest, quality, agents, aggregate, strategies, portfolio, risk, report), with CLI interface, formatted CI-style output, pipeline_runs DB persistence, and Makefile targets.

Purpose: This is the central orchestration entry point that ties all v2.0 components together into a single executable daily workflow. Without this, each component must be run manually in the correct order.

Output: scripts/daily_run.py CLI, src/pipeline/ package with DailyPipeline class, Alembic migration for pipeline_runs table, unit tests, Makefile targets.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/13-pipeline-llm-dashboard-api-tests/13-CONTEXT.md
@.planning/phases/12-portfolio-construction-risk-management/12-01-SUMMARY.md
@.planning/phases/12-portfolio-construction-risk-management/12-03-SUMMARY.md
@src/agents/registry.py
@src/strategies/__init__.py
@src/portfolio/__init__.py
@src/risk/__init__.py
@src/backtesting/__init__.py
@src/core/config.py
@src/core/database.py
@Makefile
</context>

<tasks>

<task type="auto">
  <name>Task 1: DailyPipeline class with 8-step orchestration and pipeline_runs migration</name>
  <files>
    src/pipeline/__init__.py
    src/pipeline/daily_pipeline.py
    alembic/versions/xxx_create_pipeline_runs.py
  </files>
  <action>
Create `src/pipeline/` package with `DailyPipeline` class implementing the 8-step sequential pipeline.

**DailyPipeline class (`src/pipeline/daily_pipeline.py`):**

1. `__init__(self, as_of_date: date, dry_run: bool = False)` — stores config, initializes step timings dict and results dict.

2. `run(self) -> PipelineResult` — executes all 8 steps in order. Each step is wrapped in timing logic. On failure, abort immediately (no partial execution per CONTEXT.md decision). Returns PipelineResult dataclass.

3. **8 steps as private methods:**
   - `_step_ingest()` — Calls connectors to refresh data for as_of_date. For now, log a placeholder since live ingestion depends on Docker services. Print step output.
   - `_step_quality()` — Run data quality checks via `src.quality`. Placeholder if DB unavailable. Print step output.
   - `_step_agents()` — Register all 5 agents (InflationAgent, MonetaryPolicyAgent, FiscalAgent, FxEquilibriumAgent, CrossAssetAgent), run via `AgentRegistry.run_all(as_of_date)`. Store agent reports. Print agent count and signal count.
   - `_step_aggregate()` — Use `SignalAggregator.aggregate()` on agent reports to produce per-asset-class consensus signals. Store aggregated signals.
   - `_step_strategies()` — Instantiate all 8 strategies from `ALL_STRATEGIES`, call `generate_signals(as_of_date)` on each. Collect all StrategyPosition outputs. Store positions.
   - `_step_portfolio()` — Feed strategy positions + aggregated signals into `PortfolioConstructor` and `CapitalAllocator`. Store portfolio target and allocation result.
   - `_step_risk()` — Compute VaR, run stress tests, check limits via `RiskMonitor.generate_report()`. Store risk report.
   - `_step_report()` — Generate formatted summary (see below). If not dry_run, persist pipeline run to DB.

4. **Step execution wrapper:** `_run_step(self, name: str, fn: Callable) -> None` — times the step, prints CI-style output: `"  [checkmark] {name}: {duration:.1f}s, {detail}"` on success, `"  [X] {name}: FAILED — {error}"` on failure. On failure, raises to abort pipeline.

5. **PipelineResult dataclass:** `run_id` (uuid), `date`, `status` (success/failed), `duration_seconds`, `step_timings` (dict[str, float]), `signal_count`, `position_count`, `regime`, `leverage`, `var_95`, `risk_alerts` (list[str]).

6. **Summary output** (`_format_summary() -> str`): CI build log style per CONTEXT.md:
   ```
   ═══ Daily Pipeline Run: 2024-01-15 ═══
     [checkmark] ingest:    2.1s
     [checkmark] quality:   1.3s
     [checkmark] agents:    12.3s  (5 agents, 23 signals)
     [checkmark] aggregate: 0.2s   (4 asset classes)
     [checkmark] strategies: 3.1s  (8 strategies, 12 positions)
     [checkmark] portfolio: 0.5s   (leverage: 1.8x)
     [checkmark] risk:      1.2s   (VaR95: -2.1%)
     [checkmark] report:    0.3s

   ─── Summary ───
   Signals: 23 | Positions: 12 | Leverage: 1.8x
   VaR (95%): -2.1% | Regime: NEUTRAL
   Risk Alerts: None
   Total: 21.0s | Status: SUCCESS
   ═══════════════════════════════════════
   ```

7. **DB persistence:** When not dry_run, persist to `pipeline_runs` table via sync SQLAlchemy session (pipeline is a batch script, not async). Fields: `id` (UUID PK), `run_date` (date), `status` (varchar), `duration_seconds` (float), `step_timings` (JSONB), `signal_count` (int), `position_count` (int), `regime` (varchar), `summary` (text), `created_at` (timestamp with tz).

**Alembic migration:**
Create migration for `pipeline_runs` table (regular table, not hypertable — low volume). Fields as described above. Use raw SQL for migration ops per project convention.

**Package init (`src/pipeline/__init__.py`):**
Export: DailyPipeline, PipelineResult.
  </action>
  <verify>
python -c "from src.pipeline import DailyPipeline, PipelineResult; print('Pipeline imports OK')"
alembic upgrade head  # migration applies
  </verify>
  <done>
DailyPipeline class exists with 8 sequential steps, PipelineResult dataclass, CI-style formatted output, pipeline_runs table created via migration, dry_run mode skips DB persistence.
  </done>
</task>

<task type="auto">
  <name>Task 2: CLI entry point, Makefile targets, and pipeline unit tests</name>
  <files>
    scripts/daily_run.py
    Makefile
    tests/test_pipeline/__init__.py
    tests/test_pipeline/test_daily_pipeline.py
  </files>
  <action>
**CLI script (`scripts/daily_run.py`):**

Create CLI entry point using argparse:
- `--date YYYY-MM-DD` — target date (default: today's calendar date per CONTEXT.md decision)
- `--dry-run` — run full computation but skip all DB persistence
- Parse args, construct `DailyPipeline(as_of_date=parsed_date, dry_run=args.dry_run)`, call `.run()`, print summary, exit 0 on success / 1 on failure.
- Add shebang `#!/usr/bin/env python3` and brief module docstring.
- Wrap in `if __name__ == "__main__":` guard.

**Makefile targets (append to existing Makefile):**

Add a `# ── Daily Pipeline ────────────────────────────────────────────────` section after the Verification section. Add these targets:

```makefile
# Run daily pipeline for today
daily:
	python scripts/daily_run.py

# Run daily pipeline in dry-run mode (no DB writes)
daily-dry:
	python scripts/daily_run.py --dry-run

# Run daily pipeline for a specific date
daily-date:
	python scripts/daily_run.py --date $(DATE)
```

Update the `.PHONY` line at the top to include `daily daily-dry daily-date`.

**Unit tests (`tests/test_pipeline/test_daily_pipeline.py`):**

Write tests that mock DB and external dependencies:

1. `test_pipeline_result_dataclass` — PipelineResult fields and defaults.
2. `test_step_timing_wrapper` — `_run_step` records duration and prints formatted output.
3. `test_dry_run_skips_persistence` — With dry_run=True, DB write is not called.
4. `test_pipeline_aborts_on_failure` — If a step raises, pipeline aborts immediately with FAILED status.
5. `test_format_summary_output` — Summary string contains expected fields (signals, positions, leverage, VaR, regime).
6. `test_cli_argparse_defaults` — Default date is today, dry_run is False.
7. `test_cli_argparse_custom` — `--date 2024-01-15 --dry-run` parses correctly.

Use `unittest.mock.patch` to mock agent registry, strategy execution, portfolio, and risk modules. Tests should NOT require a running database.
  </action>
  <verify>
python scripts/daily_run.py --help  # shows usage with --date and --dry-run
pytest tests/test_pipeline/ -v  # all tests pass
make -n daily-dry  # shows correct command
  </verify>
  <done>
CLI script accepts --date and --dry-run arguments, Makefile has daily/daily-dry/daily-date targets, 7+ pipeline tests pass without database dependency.
  </done>
</task>

</tasks>

<verification>
1. `python scripts/daily_run.py --help` shows --date and --dry-run options
2. `python -c "from src.pipeline import DailyPipeline, PipelineResult"` succeeds
3. `pytest tests/test_pipeline/ -v` — all tests pass
4. `make -n daily-dry` shows `python scripts/daily_run.py --dry-run`
5. Pipeline_runs migration exists in alembic/versions/
</verification>

<success_criteria>
- DailyPipeline executes 8 steps in order: ingest → quality → agents → aggregate → strategies → portfolio → risk → report
- CLI supports --date YYYY-MM-DD and --dry-run flags
- Each step prints CI-style timing output with pass/fail indicators
- Pipeline aborts immediately on any step failure
- End-of-run summary shows signal count, positions, leverage, VaR, regime, risk alerts
- Pipeline run metadata persists to pipeline_runs table (skipped in dry-run)
- Makefile targets: daily, daily-dry, daily-date
- All pipeline tests pass
</success_criteria>

<output>
After completion, create `.planning/phases/13-pipeline-llm-dashboard-api-tests/13-01-SUMMARY.md`
</output>
