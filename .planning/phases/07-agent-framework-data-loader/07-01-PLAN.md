---
phase: 07-agent-framework-data-loader
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/core/enums.py
  - src/agents/__init__.py
  - src/agents/base.py
  - src/agents/data_loader.py
  - pyproject.toml
autonomous: true
requirements: [AGENT-01, AGENT-02, AGENT-03, AGENT-04]

must_haves:
  truths:
    - "BaseAgent enforces load_data -> compute_features -> run_models -> generate_narrative pipeline via concrete run() and backtest_run() methods"
    - "AgentSignal captures signal_id, direction, strength, confidence, value, horizon_days, and metadata"
    - "PointInTimeDataLoader queries macro_series with release_time <= as_of_date and returns pandas DataFrames"
    - "PointInTimeDataLoader queries curves, market_data, and flow_data with appropriate date proxies"
    - "statsmodels and scikit-learn are installable and importable"
  artifacts:
    - path: "src/agents/base.py"
      provides: "BaseAgent ABC, AgentSignal dataclass, AgentReport dataclass"
      min_lines: 120
    - path: "src/agents/data_loader.py"
      provides: "PointInTimeDataLoader with PIT queries for 4 tables"
      min_lines: 100
    - path: "src/core/enums.py"
      provides: "SignalDirection and SignalStrength enums added"
      contains: "SignalDirection"
    - path: "src/agents/__init__.py"
      provides: "Public API re-exports for agent framework"
  key_links:
    - from: "src/agents/base.py"
      to: "src/core/enums.py"
      via: "import SignalDirection, SignalStrength"
      pattern: "from src\\.core\\.enums import.*SignalDirection"
    - from: "src/agents/data_loader.py"
      to: "src/core/database.py"
      via: "import sync_session_factory"
      pattern: "from src\\.core\\.database import.*sync_session_factory"
    - from: "src/agents/data_loader.py"
      to: "src/core/models"
      via: "import MacroSeries, CurveData, MarketData, FlowData"
      pattern: "from src\\.core\\.models"
    - from: "src/agents/base.py"
      to: "src/agents/data_loader.py"
      via: "PointInTimeDataLoader available for agent subclasses"
      pattern: "PointInTimeDataLoader"
---

<objective>
Build the foundational agent infrastructure: BaseAgent abstract class (Template Method pattern), typed signal/report dataclasses with enums, PointInTimeDataLoader for point-in-time-correct data access, and install quantitative modeling dependencies.

Purpose: All 5 analytical agents (Phases 8-10) inherit from BaseAgent, use AgentSignal/AgentReport for output, and load data through PointInTimeDataLoader. This plan establishes the contract.
Output: src/agents/base.py, src/agents/data_loader.py, updated src/core/enums.py, updated pyproject.toml
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/07-agent-framework-data-loader/07-RESEARCH.md

@src/connectors/base.py
@src/core/enums.py
@src/core/database.py
@src/core/models/signals.py
@src/core/models/macro_series.py
@src/core/models/curves.py
@src/core/models/market_data.py
@src/core/models/flow_data.py
@src/core/models/series_metadata.py
@src/core/models/instruments.py
@pyproject.toml
</context>

<tasks>

<task type="auto">
  <name>Task 1: Add SignalDirection/SignalStrength enums, install dependencies, create BaseAgent ABC with AgentSignal/AgentReport dataclasses</name>
  <files>
    src/core/enums.py
    src/agents/__init__.py
    src/agents/base.py
    pyproject.toml
  </files>
  <action>
**Step 1: Update pyproject.toml** -- Add statsmodels and scikit-learn to the `dependencies` list:
```
"statsmodels>=0.14",
"scikit-learn>=1.4",
"pandas>=2.1",
"numpy>=1.26",
"scipy>=1.12",
```
Note: pandas, numpy, scipy may already be transitive dependencies from yfinance but add them explicitly if not already present. Check the existing list first -- pandas is NOT listed as a direct dependency, only comes via yfinance transitively. Add `"pandas>=2.1"`, `"numpy>=1.26"`, `"scipy>=1.12"` as explicit dependencies alongside statsmodels and scikit-learn. Do NOT duplicate entries that already exist.

Then run: `pip install -e ".[dev]"` to install the new dependencies.

**Step 2: Update src/core/enums.py** -- Append two new enums at the bottom of the file. Follow the existing `(str, Enum)` mixin pattern exactly:

```python
class SignalDirection(str, Enum):
    """Direction of an agent's trading signal."""
    LONG = "LONG"
    SHORT = "SHORT"
    NEUTRAL = "NEUTRAL"

class SignalStrength(str, Enum):
    """Confidence-bucketed signal strength."""
    STRONG = "STRONG"         # confidence >= 0.75
    MODERATE = "MODERATE"     # 0.50 <= confidence < 0.75
    WEAK = "WEAK"             # 0.25 <= confidence < 0.50
    NO_SIGNAL = "NO_SIGNAL"   # confidence < 0.25
```

Do NOT modify existing enums. Only append.

**Step 3: Create src/agents/__init__.py** -- This is currently missing (glob found no files under src/agents/). Create it as:
```python
"""Agent framework for the Macro Trading system.

Re-exports the core agent infrastructure:
- BaseAgent: Abstract base class with Template Method pattern
- AgentSignal: Typed signal output dataclass
- AgentReport: Complete agent run output
- PointInTimeDataLoader: PIT-correct data access layer
"""

from src.agents.base import AgentReport, AgentSignal, BaseAgent
from src.agents.data_loader import PointInTimeDataLoader

__all__ = [
    "BaseAgent",
    "AgentSignal",
    "AgentReport",
    "PointInTimeDataLoader",
]
```

**Step 4: Create src/agents/base.py** -- Implement the agent framework core. Follow the BaseConnector pattern from `src/connectors/base.py` for structure (ABC with concrete orchestrator method, structlog binding):

1. **Imports**: `abc`, `dataclasses` (dataclass, field), `datetime` (date, datetime), `typing` (Any), `json`, `structlog`, enums from `src.core.enums` (SignalDirection, SignalStrength), `pg_insert` from `sqlalchemy.dialects.postgresql`, `async_session_factory` from `src.core.database`, `Signal` from `src.core.models.signals`.

2. **AgentSignal dataclass**:
   - `signal_id: str` -- e.g., "INFLATION_BR_PHILLIPS"
   - `agent_id: str` -- e.g., "inflation_agent"
   - `timestamp: datetime` -- when signal was generated (datetime.utcnow())
   - `as_of_date: date` -- point-in-time reference date
   - `direction: SignalDirection` -- LONG, SHORT, NEUTRAL
   - `strength: SignalStrength` -- STRONG, MODERATE, WEAK, NO_SIGNAL
   - `confidence: float` -- 0.0 to 1.0
   - `value: float` -- numerical signal value (z-score, model output)
   - `horizon_days: int` -- signal horizon (21=1M, 63=1Q, 252=1Y)
   - `metadata: dict = field(default_factory=dict)` -- model-specific details

3. **AgentReport dataclass**:
   - `agent_id: str`
   - `as_of_date: date`
   - `generated_at: datetime`
   - `signals: list[AgentSignal]`
   - `narrative: str` -- human-readable analysis summary
   - `model_diagnostics: dict = field(default_factory=dict)` -- model fit stats, etc.
   - `data_quality_flags: list[str] = field(default_factory=list)` -- data issues encountered

4. **Helper function `classify_strength(confidence: float) -> SignalStrength`**:
   - `>= 0.75` -> STRONG
   - `>= 0.50` -> MODERATE
   - `>= 0.25` -> WEAK
   - `< 0.25` -> NO_SIGNAL

5. **BaseAgent(abc.ABC)**:
   - `__init__(self, agent_id: str, agent_name: str)`: Store agent_id, agent_name. Create `self.log = structlog.get_logger().bind(agent=agent_id)`.
   - **Abstract methods** (all take data dict or features dict):
     - `load_data(self, as_of_date: date) -> dict[str, Any]` -- must only use data with release_time <= as_of_date
     - `compute_features(self, data: dict) -> dict[str, Any]` -- transform raw data into model features
     - `run_models(self, features: dict) -> list[AgentSignal]` -- execute quantitative models
     - `generate_narrative(self, signals: list[AgentSignal], features: dict) -> str` -- human-readable analysis
   - **Concrete `run(self, as_of_date: date) -> AgentReport`** (Template Method):
     1. Log start: `self.log.info("agent_run_start", as_of_date=str(as_of_date))`
     2. `start = datetime.utcnow()`
     3. `data = self.load_data(as_of_date)`
     4. `data_flags = self._check_data_quality(data)`
     5. `features = self.compute_features(data)`
     6. `signals = self.run_models(features)`
     7. `narrative = self.generate_narrative(signals, features)`
     8. `self._persist_signals(signals)` -- calls the async persistence
     9. Log complete: `self.log.info("agent_run_complete", signals=len(signals), elapsed=(datetime.utcnow()-start).total_seconds())`
     10. Return `AgentReport(agent_id, as_of_date, datetime.utcnow(), signals, narrative, {}, data_flags)`
   - **Concrete `backtest_run(self, as_of_date: date) -> AgentReport`**: Same as run() but does NOT call `_persist_signals()`. Does NOT log ingestion metrics. Returns AgentReport with empty data_quality_flags.
   - **Concrete `_check_data_quality(self, data: dict) -> list[str]`**: Iterate over data dict. For values that are pandas DataFrames, check `df.isna().sum().sum() > 0` and append flag string. For values that are None, append `"{key}: data is None"`. Return list of flag strings.
   - **Concrete `_persist_signals(self, signals: list[AgentSignal]) -> int`**: Convert AgentSignal list to records for the Signal model. For each signal:
     - `signal_type` = signal.signal_id
     - `signal_date` = signal.as_of_date
     - `instrument_id` = None (agent signals are not instrument-specific)
     - `series_id` = None
     - `value` = signal.value
     - `confidence` = signal.confidence
     - `metadata_json` = `json.dumps({"direction": signal.direction.value, "strength": signal.strength.value, "horizon_days": signal.horizon_days, "agent_id": signal.agent_id, **signal.metadata})`

     Use `asyncio.get_event_loop().run_until_complete()` to call the async bulk insert if no event loop is running, or `asyncio.run()` for a new loop. Actually, since BaseConnector._bulk_insert is async but agents may run in sync context (backtesting), use this approach:
     ```python
     import asyncio
     async def _persist_signals_async(self, signals):
         records = [...]  # build records list
         if not records:
             return 0
         async with async_session_factory() as session:
             async with session.begin():
                 stmt = pg_insert(Signal).values(records)
                 stmt = stmt.on_conflict_do_nothing(constraint="uq_signals_natural_key")
                 result = await session.execute(stmt)
                 return result.rowcount

     def _persist_signals(self, signals):
         try:
             loop = asyncio.get_running_loop()
             # Already in async context -- create a task
             import concurrent.futures
             with concurrent.futures.ThreadPoolExecutor() as pool:
                 future = pool.submit(asyncio.run, self._persist_signals_async(signals))
                 return future.result()
         except RuntimeError:
             # No running loop -- safe to use asyncio.run
             return asyncio.run(self._persist_signals_async(signals))
     ```
     Log the result: `self.log.info("signals_persisted", count=inserted)`

All type hints required. Docstrings on every public method/class.
  </action>
  <verify>
Run these checks in sequence:
1. `cd /home/user/Macro_Trading && pip install -e ".[dev]" 2>&1 | tail -5` -- must succeed
2. `python -c "import statsmodels; import sklearn; print('deps OK')"` -- must print "deps OK"
3. `python -c "from src.core.enums import SignalDirection, SignalStrength; print(SignalDirection.LONG.value, SignalStrength.STRONG.value)"` -- must print "LONG STRONG"
4. `python -c "from src.agents.base import BaseAgent, AgentSignal, AgentReport, classify_strength; print(classify_strength(0.8))"` -- must print "SignalStrength.STRONG"
5. `python -c "from src.agents import BaseAgent, AgentSignal, AgentReport, PointInTimeDataLoader; print('imports OK')"` -- must print "imports OK"
6. `ruff check src/core/enums.py src/agents/base.py src/agents/__init__.py` -- no errors
  </verify>
  <done>
SignalDirection and SignalStrength enums exist in src/core/enums.py. AgentSignal dataclass has all 10 fields (signal_id, agent_id, timestamp, as_of_date, direction, strength, confidence, value, horizon_days, metadata). AgentReport dataclass has all 7 fields. BaseAgent ABC has 4 abstract methods (load_data, compute_features, run_models, generate_narrative) and 4 concrete methods (run, backtest_run, _check_data_quality, _persist_signals). classify_strength maps confidence to SignalStrength correctly. statsmodels and scikit-learn import successfully.
  </done>
</task>

<task type="auto">
  <name>Task 2: Create PointInTimeDataLoader with PIT-correct queries for all 4 data tables</name>
  <files>
    src/agents/data_loader.py
  </files>
  <action>
Create `src/agents/data_loader.py` -- the single data access layer for ALL agents, both for live execution and backtesting.

**Design decisions (from research):**
- Use **sync sessions** (psycopg2) via `sync_session_factory` -- agent runs are batch processes, not concurrent web requests.
- `macro_series` has `release_time` NOT NULL -- direct PIT filtering.
- `curves` has NO `release_time` -- use `curve_date <= as_of_date` as proxy (curves published same day).
- `market_data` has NO `release_time` -- use `timestamp <= as_of_date` as proxy (prices available real-time).
- `flow_data` has `release_time` nullable -- use when available, fall back to `observation_date`.
- Return pandas DataFrames with meaningful column names.
- Session lifecycle: open/close per method call (no long-lived session).

**Imports:**
```python
from datetime import date, timedelta
from typing import Optional
import pandas as pd
import structlog
from sqlalchemy import select, and_, func, cast, Date
from src.core.database import sync_session_factory
from src.core.models.macro_series import MacroSeries
from src.core.models.curves import CurveData
from src.core.models.market_data import MarketData
from src.core.models.flow_data import FlowData
from src.core.models.series_metadata import SeriesMetadata
from src.core.models.instruments import Instrument
```

**Class PointInTimeDataLoader:**

Constructor: `self.log = structlog.get_logger().bind(component="pit_data_loader")`

**Method 1: `get_macro_series(self, series_code: str, as_of_date: date, lookback_days: int = 3650) -> pd.DataFrame`**
- Joins `macro_series` with `series_metadata` to resolve series_code to series_id.
- Query: `WHERE sm.series_code = :series_code AND ms.release_time <= :as_of_date AND ms.observation_date >= :start_date`
- For revised series, use a subquery to get only the latest revision available at as_of_date: `GROUP BY observation_date` then take `MAX(revision_number) WHERE release_time <= as_of_date`. Actually simpler: order by (observation_date, revision_number DESC) and use distinct on observation_date, but SQLAlchemy standard approach is:
  ```python
  start = as_of_date - timedelta(days=lookback_days)
  stmt = (
      select(
          MacroSeries.observation_date.label("date"),
          MacroSeries.value,
          MacroSeries.release_time,
          MacroSeries.revision_number,
      )
      .join(SeriesMetadata, MacroSeries.series_id == SeriesMetadata.id)
      .where(and_(
          SeriesMetadata.series_code == series_code,
          cast(MacroSeries.release_time, Date) <= as_of_date,
          MacroSeries.observation_date >= start,
      ))
      .order_by(MacroSeries.observation_date, MacroSeries.revision_number.desc())
  )
  ```
  Then in Python, deduplicate by observation_date keeping only the first (highest revision) row.
- Return: DataFrame with columns ["date", "value", "release_time", "revision_number"]. Set "date" as DatetimeIndex.
- If no data found, return empty DataFrame with same columns.
- Log: `self.log.debug("macro_series_loaded", series=series_code, rows=len(df), as_of=str(as_of_date))`

**Method 2: `get_latest_macro_value(self, series_code: str, as_of_date: date) -> Optional[float]`**
- Same PIT filtering as get_macro_series but `LIMIT 1 ORDER BY observation_date DESC`.
- Return the float value, or None if no data.

**Method 3: `get_curve(self, curve_id: str, as_of_date: date) -> dict[int, float]`**
- Query: `WHERE curve_id = :curve_id AND curve_date <= :as_of_date ORDER BY curve_date DESC`
- Get the most recent curve_date, then load ALL tenor points for that date.
- Step 1: Find max curve_date <= as_of_date for this curve_id.
- Step 2: Load all rows for that (curve_id, curve_date) combination.
- Return: `{tenor_days: rate}` dict. Empty dict if no data.
- Log: `self.log.debug("curve_loaded", curve_id=curve_id, tenors=len(result), curve_date=str(found_date), as_of=str(as_of_date))`

**Method 4: `get_curve_history(self, curve_id: str, tenor_days: int, as_of_date: date, lookback_days: int = 756) -> pd.DataFrame`**
- Query: `WHERE curve_id = :curve_id AND tenor_days = :tenor_days AND curve_date <= :as_of_date AND curve_date >= :start ORDER BY curve_date`
- Return: DataFrame with columns ["date", "rate"]. Set "date" as DatetimeIndex.

**Method 5: `get_market_data(self, ticker: str, as_of_date: date, lookback_days: int = 756) -> pd.DataFrame`**
- Joins `market_data` with `instruments` to resolve ticker to instrument_id.
- Query: `WHERE i.ticker = :ticker AND md.timestamp <= :as_of_date_end_of_day AND md.timestamp >= :start`
- as_of_date_end_of_day: use `datetime.combine(as_of_date, datetime.max.time())` to include all intraday data on as_of_date.
- Return: DataFrame with columns ["date", "open", "high", "low", "close", "volume", "adjusted_close"]. Set "date" as DatetimeIndex.
- If no data, return empty DataFrame.

**Method 6: `get_flow_data(self, series_code: str, as_of_date: date, lookback_days: int = 365) -> pd.DataFrame`**
- Joins `flow_data` with `series_metadata` to resolve series_code.
- PIT logic: `WHERE (fd.release_time IS NOT NULL AND fd.release_time <= :as_of_date) OR (fd.release_time IS NULL AND fd.observation_date <= :as_of_date)`
- Also: `fd.observation_date >= :start`
- Return: DataFrame with columns ["date", "value", "flow_type", "release_time"].

**Method 7: `get_focus_expectations(self, indicator: str, as_of_date: date, lookback_days: int = 365) -> pd.DataFrame`**
- Queries macro_series for Focus survey data. Focus series have series_codes like "BR_FOCUS_IPCA_CY_MEDIAN".
- Uses same PIT logic as get_macro_series.
- This is a convenience wrapper that calls `get_macro_series(f"BR_FOCUS_{indicator}_CY_MEDIAN", as_of_date, lookback_days)`.
- Return: DataFrame.

**Session management pattern for every method:**
```python
session = sync_session_factory()
try:
    result = session.execute(stmt)
    rows = result.all()
    # ... build DataFrame
finally:
    session.close()
```

All methods must have type hints and docstrings explaining the PIT semantics.
  </action>
  <verify>
Run these checks:
1. `python -c "from src.agents.data_loader import PointInTimeDataLoader; loader = PointInTimeDataLoader(); print('loader created')"` -- must succeed
2. `python -c "
from src.agents.data_loader import PointInTimeDataLoader
import inspect
loader = PointInTimeDataLoader()
methods = [m for m in dir(loader) if not m.startswith('_')]
expected = ['get_curve', 'get_curve_history', 'get_flow_data', 'get_focus_expectations', 'get_latest_macro_value', 'get_macro_series', 'get_market_data']
for e in expected:
    assert e in methods, f'Missing method: {e}'
print('all 7 methods present')
"` -- must print "all 7 methods present"
3. `ruff check src/agents/data_loader.py` -- no errors
4. `python -c "
from src.agents.data_loader import PointInTimeDataLoader
from datetime import date
loader = PointInTimeDataLoader()
# Test with a known series -- BR_SELIC_TARGET should have data from backfill
df = loader.get_macro_series('BR_SELIC_TARGET', date(2024, 6, 15), lookback_days=365)
print(f'BR_SELIC_TARGET rows: {len(df)}, cols: {list(df.columns)}')
"` -- should show rows > 0 and correct columns (requires DB running with data)
  </verify>
  <done>
PointInTimeDataLoader has 7 methods: get_macro_series (with release_time PIT filtering), get_latest_macro_value, get_curve (curve_date proxy), get_curve_history, get_market_data (timestamp proxy), get_flow_data (nullable release_time with fallback), get_focus_expectations. All use sync sessions. All return pandas DataFrames (or dict/float for specific methods). PIT semantics documented in docstrings. Imports verified.
  </done>
</task>

</tasks>

<verification>
After both tasks complete:
1. `python -c "from src.agents import BaseAgent, AgentSignal, AgentReport, PointInTimeDataLoader; print('All exports OK')"` -- framework importable
2. `python -c "import statsmodels; import sklearn; print('ML deps OK')"` -- dependencies installed
3. `python -c "
from src.agents.base import BaseAgent, AgentSignal, classify_strength
from src.core.enums import SignalDirection, SignalStrength
from datetime import date, datetime
# Test dataclass creation
sig = AgentSignal(
    signal_id='TEST_SIGNAL', agent_id='test', timestamp=datetime.utcnow(),
    as_of_date=date.today(), direction=SignalDirection.LONG,
    strength=SignalStrength.STRONG, confidence=0.85, value=1.5,
    horizon_days=63, metadata={'model': 'test'}
)
assert sig.confidence == 0.85
assert sig.direction == SignalDirection.LONG
assert classify_strength(0.85) == SignalStrength.STRONG
assert classify_strength(0.6) == SignalStrength.MODERATE
assert classify_strength(0.3) == SignalStrength.WEAK
assert classify_strength(0.1) == SignalStrength.NO_SIGNAL
print('All assertions passed')
"` -- dataclass and enum logic correct
4. `ruff check src/agents/ src/core/enums.py` -- no lint errors
</verification>

<success_criteria>
1. BaseAgent is an ABC with 4 abstract methods and 4 concrete methods (run, backtest_run, _check_data_quality, _persist_signals)
2. AgentSignal has all 10 fields with correct types
3. AgentReport has all 7 fields
4. SignalDirection and SignalStrength enums are in src/core/enums.py following (str, Enum) pattern
5. PointInTimeDataLoader has 7 methods querying 4 tables with PIT correctness
6. statsmodels and scikit-learn install and import successfully
7. All code passes ruff linting
</success_criteria>

<output>
After completion, create `.planning/phases/07-agent-framework-data-loader/07-01-SUMMARY.md`
</output>
