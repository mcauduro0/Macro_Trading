---
phase: 27-redis-cache-dagster-pms-go-live-verification
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - src/cache/__init__.py
  - src/cache/pms_cache.py
  - src/api/routes/pms_portfolio.py
  - src/api/routes/pms_briefing.py
  - src/api/routes/pms_risk.py
  - src/api/routes/pms_attribution.py
  - src/api/routes/pms_trades.py
autonomous: true
requirements:
  - PMS-CACHE-01
  - PMS-CACHE-02

must_haves:
  truths:
    - "PMS GET endpoints return cached data on second request within TTL window"
    - "Write operations (approve, close, MTM) immediately refresh cache with fresh data"
    - "Different PMS endpoints have tiered TTLs matching data volatility"
    - "Cache invalidation cascades correctly (position change invalidates book + risk + attribution)"
  artifacts:
    - path: "src/cache/pms_cache.py"
      provides: "PMSCache class with get/set/invalidate for 4 endpoint types"
      contains: "class PMSCache"
    - path: "src/cache/__init__.py"
      provides: "Package init exporting PMSCache"
  key_links:
    - from: "src/api/routes/pms_portfolio.py"
      to: "src/cache/pms_cache.py"
      via: "PMSCache.get_book / set_book on GET /book"
      pattern: "cache\\.get_book|cache\\.set_book"
    - from: "src/api/routes/pms_portfolio.py"
      to: "src/cache/pms_cache.py"
      via: "PMSCache.invalidate_portfolio_data on write endpoints"
      pattern: "invalidate_portfolio_data"
    - from: "src/api/routes/pms_risk.py"
      to: "src/cache/pms_cache.py"
      via: "PMSCache.get_risk_metrics / set_risk_metrics"
      pattern: "cache\\.get_risk|cache\\.set_risk"
---

<objective>
Redis caching layer for all PMS query endpoints with write-through + invalidate pattern.

Purpose: Eliminate cold-cache latency for the portfolio manager's daily workflow. The manager sees fresh data on first login because the Dagster pipeline warms the cache (Plan 27-02). During the day, writes immediately refresh the cache so subsequent reads are instant.

Output: `src/cache/pms_cache.py` with PMSCache class, integrated into all 6 PMS API route files with cache-first reads and write-through invalidation.
</objective>

<execution_context>
@./.claude/get-shit-done/workflows/execute-plan.md
@./.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md

@src/core/redis.py
@src/api/routes/pms_portfolio.py
@src/api/routes/pms_briefing.py
@src/api/routes/pms_risk.py
@src/api/routes/pms_attribution.py
@src/api/routes/pms_trades.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: PMSCache class with tiered TTLs and cascade invalidation</name>
  <files>src/cache/__init__.py, src/cache/pms_cache.py</files>
  <action>
Create `src/cache/__init__.py` exporting PMSCache and a `get_pms_cache` FastAPI dependency.

Create `src/cache/pms_cache.py` with class `PMSCache`:
- Constructor takes a `redis.asyncio.Redis` client (from `src.core.redis.get_redis`)
- Key prefix: `pms:` for all keys
- Tiered TTLs per locked decision and guide spec:
  - Book (positions): 30 seconds (near real-time)
  - Risk metrics: 60 seconds
  - Morning pack briefing: 300 seconds (5 min)
  - Attribution: 300 seconds (5 min)

Methods (all async):
- `get_book() -> dict | None` — get from `pms:book`
- `set_book(data: dict, ttl: int = 30)` — JSON serialize and SET with EX
- `get_morning_pack(date_key: str) -> dict | None` — get from `pms:morning_pack:{date_key}`
- `set_morning_pack(date_key: str, data: dict, ttl: int = 300)`
- `get_risk_metrics() -> dict | None` — get from `pms:risk:live`
- `set_risk_metrics(data: dict, ttl: int = 60)`
- `get_attribution(period_key: str) -> dict | None` — get from `pms:attribution:{period_key}`
- `set_attribution(period_key: str, data: dict, ttl: int = 300)`
- `invalidate_portfolio_data()` — cascade invalidation: delete book, risk, and all attribution keys (use SCAN for pattern `pms:attribution:*`)
- `refresh_book(book_data: dict)` — write-through: set_book with fresh data (per locked decision: proactive refresh, not just delete)
- `refresh_risk(risk_data: dict)` — write-through: set_risk_metrics with fresh data

JSON serialization: use `json.dumps` with `default=str` for datetime handling. Deserialization: `json.loads`.

Create `get_pms_cache` async dependency function:
```python
async def get_pms_cache() -> PMSCache:
    redis = await get_redis()
    return PMSCache(redis)
```

Export both `PMSCache` and `get_pms_cache` from `__init__.py`.
  </action>
  <verify>
Run: `python -c "from src.cache import PMSCache, get_pms_cache; c = PMSCache.__new__(PMSCache); print('OK:', [m for m in dir(c) if not m.startswith('_')])"` -- should list get_book, set_book, get_morning_pack, etc.
  </verify>
  <done>PMSCache class importable with all 10 methods (get/set for 4 endpoints + invalidate_portfolio_data + refresh_book + refresh_risk), tiered TTLs configured, get_pms_cache dependency available</done>
</task>

<task type="auto">
  <name>Task 2: Wire cache into PMS API routes with read-through and write-through</name>
  <files>src/api/routes/pms_portfolio.py, src/api/routes/pms_briefing.py, src/api/routes/pms_risk.py, src/api/routes/pms_attribution.py, src/api/routes/pms_trades.py</files>
  <action>
Update all 5 PMS route files to integrate caching. Import `get_pms_cache` from `src.cache` and add as FastAPI `Depends()` parameter.

**pms_portfolio.py** (read + write):
- GET `/book` endpoint: try `cache.get_book()` first. If hit, return with `"cached": True, "cache_age_seconds": ttl_remaining`. If miss, compute via PositionManager.get_book(), then `cache.set_book(result)` before returning.
- POST endpoints that modify positions (open, close, MTM): after DB write completes, call `cache.invalidate_portfolio_data()` then `cache.refresh_book(new_book_data)` to write-through. Recompute book from PositionManager and refresh.

**pms_briefing.py** (read + write):
- GET `/latest` or GET `/{date}` endpoint: try `cache.get_morning_pack(date_key)`. If hit, return cached. If miss, compute and `cache.set_morning_pack(date_key, result)`.
- POST `/generate` endpoint: after generating, set cache for today's date.

**pms_risk.py** (read):
- GET `/live` endpoint: try `cache.get_risk_metrics()`. If hit, return cached. If miss, compute and `cache.set_risk_metrics(result)`.

**pms_attribution.py** (read):
- GET endpoint: derive period_key from query params (e.g., `"daily_2026-02-25"` or `"mtd_2026-02"`). Try `cache.get_attribution(period_key)`. If miss, compute and `cache.set_attribution(period_key, result)`.

**pms_trades.py** (write):
- POST approve/reject endpoints: after DB write, call `cache.invalidate_portfolio_data()` since approving a trade changes the book.

Pattern for all cache reads: wrap in try/except so Redis failure degrades gracefully (log warning, proceed without cache). Never let Redis errors break the endpoint.

Add `import logging; logger = logging.getLogger(__name__)` and log cache hits/misses at DEBUG level.
  </action>
  <verify>
Run: `python -c "from src.api.main import app; routes = [r.path for r in app.routes if hasattr(r, 'path')]; pms = [r for r in routes if '/pms/' in r]; print(f'{len(pms)} PMS routes loaded')"` -- should show 20+ PMS routes still functional after cache integration.
  </verify>
  <done>All 5 PMS route files use PMSCache: GET endpoints do cache-first reads, write endpoints do write-through + invalidate. Redis failures degrade gracefully without breaking endpoints.</done>
</task>

</tasks>

<verification>
1. `python -c "from src.cache import PMSCache, get_pms_cache; print('Cache module OK')"` passes
2. `python -c "from src.api.main import app; print('App loads OK')"` passes (no import errors from cache wiring)
3. All PMS GET endpoints have cache-first read pattern
4. All PMS write endpoints call invalidate_portfolio_data or refresh methods
5. Graceful degradation: try/except around all Redis calls in routes
</verification>

<success_criteria>
- PMSCache class exists with 4 endpoint types, tiered TTLs (30s/60s/300s/300s)
- Write-through + invalidate pattern: writes refresh cache immediately (not lazy)
- All 6 PMS route files integrated with cache
- Redis failures do not break API endpoints
</success_criteria>

<output>
After completion, create `.planning/phases/27-redis-cache-dagster-pms-go-live-verification/27-01-SUMMARY.md`
</output>
